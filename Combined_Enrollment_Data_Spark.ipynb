{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Enrollment and Contract Data\n",
    "#### 1. Import libraries & Create a Spark Session\n",
    "#### 2. Read CSVs into Spark DataFrames\n",
    "#### 3. Merge DataFrames: make inner join on \"Plan ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "#import dask.dataframe as dd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Enrollment_CSV_Merge\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test file location\n",
    "contract_2013_01_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_01.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2013 CSV files into DataFrames\n",
    "contract_2013_01_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_01.csv\", header=True, inferSchema=True)\n",
    "contract_2013_02_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_02.csv\", header=True, inferSchema=True)\n",
    "contract_2013_03_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_03.csv\", header=True, inferSchema=True)\n",
    "contract_2013_04_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_04.csv\", header=True, inferSchema=True)\n",
    "contract_2013_05_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_05.csv\", header=True, inferSchema=True)\n",
    "contract_2013_06_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_06.csv\", header=True, inferSchema=True)\n",
    "contract_2013_07_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_07.csv\", header=True, inferSchema=True)\n",
    "contract_2013_08_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_08.csv\", header=True, inferSchema=True)\n",
    "contract_2013_09_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_09.csv\", header=True, inferSchema=True)\n",
    "contract_2013_10_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_10.csv\", header=True, inferSchema=True)\n",
    "contract_2013_11_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_11.csv\", header=True, inferSchema=True)\n",
    "contract_2013_12_df = spark.read.csv(\"./2013/CPSC_Contract_Info_2013_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_01_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_02_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_03_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_04_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_05_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_06_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_07_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_08_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_09_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_10_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_11_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2013_12_df = spark.read.csv(\"./2013/CPSC_Enrollment_Info_2013_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2013_01_df.printSchema()\n",
    "enrollment_2013_01_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+---------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+-------------+----------+\n",
      "|Plan ID|Contract ID|Organization Type|Plan Type|Offers Part D|SNP Plan|EGHP|   Organization Name|Organization Marketing Name|           Plan Name| Parent Organization|Contract Effective Date|Contract Number|SSA State County Code|FIPS State County Code|State|       County|Enrollment|\n",
      "+-------+-----------+-----------------+---------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+-------------+----------+\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16000|                 19001|   IA|        Adair|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16010|                 19003|   IA|        Adams|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16030|                 19007|   IA|    Appanoose|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16050|                 19011|   IA|       Benton|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16070|                 19015|   IA|        Boone|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16140|                 19029|   IA|         Cass|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16150|                 19031|   IA|        Cedar|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16190|                 19039|   IA|       Clarke|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16220|                 19045|   IA|      Clinton|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16230|                 19047|   IA|     Crawford|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16250|                 19051|   IA|        Davis|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16280|                 19057|   IA|   Des Moines|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16350|                 19071|   IA|      Fremont|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16360|                 19073|   IA|       Greene|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16370|                 19075|   IA|       Grundy|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16380|                 19077|   IA|      Guthrie|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16410|                 19083|   IA|       Hardin|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16420|                 19085|   IA|     Harrison|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16430|                 19087|   IA|        Henry|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16470|                 19095|   IA|         Iowa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16490|                 19099|   IA|       Jasper|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16500|                 19101|   IA|    Jefferson|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16510|                 19103|   IA|      Johnson|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16530|                 19107|   IA|       Keokuk|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16550|                 19111|   IA|          Lee|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16570|                 19115|   IA|       Louisa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16580|                 19117|   IA|        Lucas|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16600|                 19121|   IA|      Madison|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16610|                 19123|   IA|      Mahaska|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16630|                 19127|   IA|     Marshall|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16640|                 19129|   IA|        Mills|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16660|                 19133|   IA|       Monona|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16680|                 19137|   IA|   Montgomery|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16690|                 19139|   IA|    Muscatine|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16760|                 19153|   IA|         Polk|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16770|                 19155|   IA|Pottawattamie|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16780|                 19157|   IA|    Poweshiek|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16790|                 19159|   IA|     Ringgold|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16810|                 19163|   IA|        Scott|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16820|                 19165|   IA|       Shelby|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16850|                 19171|   IA|         Tama|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16880|                 19177|   IA|    Van Buren|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16900|                 19181|   IA|       Warren|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16910|                 19183|   IA|   Washington|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0108|                 3090|                  4019|   AZ|         Pima|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0108|                 5200|                  6037|   CA|  Los Angeles|        35|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0108|                 5210|                  6037|   CA|  Los Angeles|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 3060|                  4013|   AZ|     Maricopa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 3100|                  4021|   AZ|        Pinal|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 3130|                  4027|   AZ|         Yuma|         *|\n",
      "+-------+-----------+-----------------+---------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+-------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2013_merged_df = contract_2013_01_df.join(enrollment_2013_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "\n",
    "# Show result\n",
    "Jan_2013_merged_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "#Jan_2013_merged_df = contract_2013_01_df.join(enrollment_2013_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2013_merged_df = contract_2013_02_df.join(enrollment_2013_02_df, on='Plan ID', how='outer')\n",
    "Mar_2013_merged_df = contract_2013_03_df.join(enrollment_2013_03_df, on='Plan ID', how='outer')\n",
    "Apr_2013_merged_df = contract_2013_04_df.join(enrollment_2013_04_df, on='Plan ID', how='outer')\n",
    "May_2013_merged_df = contract_2013_05_df.join(enrollment_2013_05_df, on='Plan ID', how='outer')\n",
    "Jun_2013_merged_df = contract_2013_06_df.join(enrollment_2013_06_df, on='Plan ID', how='outer')\n",
    "Jul_2013_merged_df = contract_2013_07_df.join(enrollment_2013_07_df, on='Plan ID', how='outer')\n",
    "Aug_2013_merged_df = contract_2013_08_df.join(enrollment_2013_08_df, on='Plan ID', how='outer')\n",
    "Sep_2013_merged_df = contract_2013_09_df.join(enrollment_2013_09_df, on='Plan ID', how='outer')\n",
    "Oct_2013_merged_df = contract_2013_10_df.join(enrollment_2013_10_df, on='Plan ID', how='outer')\n",
    "Nov_2013_merged_df = contract_2013_11_df.join(enrollment_2013_11_df, on='Plan ID', how='outer')\n",
    "Dec_2013_merged_df = contract_2013_12_df.join(enrollment_2013_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2013\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2013_merged_df = Jan_2013_merged_df.drop(*columns_to_drop)\n",
    "Feb_2013_merged_df = Feb_2013_merged_df.drop(*columns_to_drop)\n",
    "Mar_2013_merged_df = Mar_2013_merged_df.drop(*columns_to_drop)\n",
    "Apr_2013_merged_df = Apr_2013_merged_df.drop(*columns_to_drop)\n",
    "May_2013_merged_df = May_2013_merged_df.drop(*columns_to_drop)\n",
    "Jun_2013_merged_df = Jun_2013_merged_df.drop(*columns_to_drop)\n",
    "Jul_2013_merged_df = Jul_2013_merged_df.drop(*columns_to_drop)\n",
    "Aug_2013_merged_df = Aug_2013_merged_df.drop(*columns_to_drop)\n",
    "Sep_2013_merged_df = Sep_2013_merged_df.drop(*columns_to_drop)\n",
    "Oct_2013_merged_df = Oct_2013_merged_df.drop(*columns_to_drop)\n",
    "Nov_2013_merged_df = Nov_2013_merged_df.drop(*columns_to_drop)\n",
    "Dec_2013_merged_df = Dec_2013_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "# Drop County after adding enrollment by state \n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+-----+---------+----------+\n",
      "|Plan ID|Contract ID|Plan Type|Offers Part D|   Organization Name|Organization Marketing Name| Parent Organization|Contract Number|FIPS State County Code|State|   County|Enrollment|\n",
      "+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+-----+---------+----------+\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19001|   IA|    Adair|         *|\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19003|   IA|    Adams|         *|\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19007|   IA|Appanoose|         *|\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19011|   IA|   Benton|         *|\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19015|   IA|    Boone|         *|\n",
      "+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+-----+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test another merged DF to ensure data combined correctly\n",
    "May_2013_merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2013_dfs = {\n",
    "    'Jan_2013': (Jan_2013_merged_df, 'January', '2013')\n",
    "    'Feb_2013': (Feb_2013_merged_df, 'February', '2013'),\n",
    "    'Mar_2013': (Mar_2013_merged_df, 'March', '2013'),\n",
    "    'Apr_2013': (Apr_2013_merged_df, 'April', '2013'),\n",
    "    'May_2013': (May_2013_merged_df, 'May', '2013'),\n",
    "    'Jun_2013': (Jun_2013_merged_df, 'June', '2013'),\n",
    "    'Jul_2013': (Jul_2013_merged_df, 'July', '2013'),\n",
    "    'Aug_2013': (Aug_2013_merged_df, 'August', '2013'),\n",
    "    'Sep_2013': (Sep_2013_merged_df, 'September', '2013'),\n",
    "    'Oct_2013': (Oct_2013_merged_df, 'October', '2013'),\n",
    "    'Nov_2013': (Nov_2013_merged_df, 'November', '2013'),\n",
    "    'Dec_2013': (Dec_2013_merged_df, 'December', '2013')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2013_dfs:\n",
    "    df, month, year = merged_2013_dfs[values]\n",
    "    merged_2013_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2013_merged_df = merged_2013_dfs['Jan_2013']\n",
    "Feb_2013_merged_df = merged_2013_dfs['Feb_2013']\n",
    "Mar_2013_merged_df = merged_2013_dfs['Mar_2013']\n",
    "Apr_2013_merged_df = merged_2013_dfs['Apr_2013']\n",
    "May_2013_merged_df = merged_2013_dfs['May_2013']\n",
    "Jun_2013_merged_df = merged_2013_dfs['Jun_2013']\n",
    "Jul_2013_merged_df = merged_2013_dfs['Jul_2013']\n",
    "Aug_2013_merged_df = merged_2013_dfs['Aug_2013']\n",
    "Sep_2013_merged_df = merged_2013_dfs['Sep_2013']\n",
    "Oct_2013_merged_df = merged_2013_dfs['Oct_2013']\n",
    "Nov_2013_merged_df = merged_2013_dfs['Nov_2013']\n",
    "Dec_2013_merged_df = merged_2013_dfs['Dec_2013']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+-----+---------+----------+-----+----+\n",
      "|Plan ID|Contract ID|Plan Type|Offers Part D|   Organization Name|Organization Marketing Name| Parent Organization|Contract Number|FIPS State County Code|State|   County|Enrollment|Month|Year|\n",
      "+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+-----+---------+----------+-----+----+\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19001|   IA|    Adair|         *| June|2013|\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19003|   IA|    Adams|         *| June|2013|\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19007|   IA|Appanoose|         *| June|2013|\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19011|   IA|   Benton|         *| June|2013|\n",
      "|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19015|   IA|    Boone|         *| June|2013|\n",
      "+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+-----+---------+----------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test a df to ensure columns were added appropriately\n",
    "Jun_2013_merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2013_dfs = {\n",
    "    'Jan_2013': Jan_2013_merged_df\n",
    "    'Feb_2013': Feb_2013_merged_df,\n",
    "    'Mar_2013': Mar_2013_merged_df,\n",
    "    'Apr_2013': Apr_2013_merged_df,\n",
    "    'May_2013': May_2013_merged_df,\n",
    "    'Jun_2013': Jun_2013_merged_df,\n",
    "    'Jul_2013': Jul_2013_merged_df,\n",
    "    'Aug_2013': Aug_2013_merged_df,\n",
    "    'Sep_2013': Sep_2013_merged_df,\n",
    "    'Oct_2013': Oct_2013_merged_df,\n",
    "    'Nov_2013': Nov_2013_merged_df,\n",
    "    'Dec_2013': Dec_2013_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2013_dfs:\n",
    "    merged_2013_dfs[values] = add_total_enrollments(merged_2013_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2013_merged_df = merged_2013_dfs['Jan_2013']\n",
    "Feb_2013_merged_df = merged_2013_dfs['Feb_2013']\n",
    "Mar_2013_merged_df = merged_2013_dfs['Mar_2013']\n",
    "Apr_2013_merged_df = merged_2013_dfs['Apr_2013']\n",
    "May_2013_merged_df = merged_2013_dfs['May_2013']\n",
    "Jun_2013_merged_df = merged_2013_dfs['Jun_2013']\n",
    "Jul_2013_merged_df = merged_2013_dfs['Jul_2013']\n",
    "Aug_2013_merged_df = merged_2013_dfs['Aug_2013']\n",
    "Sep_2013_merged_df = merged_2013_dfs['Sep_2013']\n",
    "Oct_2013_merged_df = merged_2013_dfs['Oct_2013']\n",
    "Nov_2013_merged_df = merged_2013_dfs['Nov_2013']\n",
    "Dec_2013_merged_df = merged_2013_dfs['Dec_2013']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+---------+----------+-----+----+--------------------------+\n",
      "|State|Plan ID|Contract ID|Plan Type|Offers Part D|   Organization Name|Organization Marketing Name| Parent Organization|Contract Number|FIPS State County Code|   County|Enrollment|Month|Year|Total Enrollments by State|\n",
      "+-----+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+---------+----------+-----+----+--------------------------+\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19001|    Adair|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19003|    Adams|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19007|Appanoose|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19011|   Benton|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19015|    Boone|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19029|     Cass|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19031|    Cedar|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19039|   Clarke|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19045|  Clinton|         *|March|2013|               7.2932232E7|\n",
      "|   IA|     31|      H0084|Local PPO|          Yes|CARE IMPROVEMENT ...|       Care Improvement ...|UnitedHealth Grou...|          H0084|                 19047| Crawford|         *|March|2013|               7.2932232E7|\n",
      "+-----+-------+-----------+---------+-------------+--------------------+---------------------------+--------------------+---------------+----------------------+---------+----------+-----+----+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test a month to view data\n",
    "Mar_2013_merged_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2013_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2013 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2013': Jan_2013_merged_df\n",
    "    'Feb_2013': Feb_2013_merged_df,\n",
    "    'Mar_2013': Mar_2013_merged_df,\n",
    "    'Apr_2013': Apr_2013_merged_df,\n",
    "    'May_2013': May_2013_merged_df,\n",
    "    'Jun_2013': Jun_2013_merged_df,\n",
    "    'Jul_2013': Jul_2013_merged_df,\n",
    "    'Aug_2013': Aug_2013_merged_df,\n",
    "    'Sep_2013': Sep_2013_merged_df,\n",
    "    'Oct_2013': Oct_2013_merged_df,\n",
    "    'Nov_2013': Nov_2013_merged_df,\n",
    "    'Dec_2013': Dec_2013_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kathr\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\kathr\\AppData\\Local\\Temp\\ipykernel_3492\\97050446.py\", line 2, in <module>\n",
      "    merged_2013_01_df.write.parquet(\"./Combined_Data_CSVs/Merged_Data_2013_01_parquet\")\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 1721, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save result using parquet\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmerged_2013_01_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Combined_Data_CSVs/Merged_Data_2013_01_parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'KeyboardInterrupt'>, KeyboardInterrupt())",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:2063\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2060\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2063\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2065\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py:545\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    539\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    540\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    542\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    546\u001b[0m }\n\u001b[0;32m    548\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# Save result using parquet\n",
    "Jan_2013_merged_df.write.parquet(\"./Combined_Data_CSVs/Merged_Data_2013_01_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2014 CSV files into DataFrames\n",
    "contract_2014_01_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_01.csv\", header=True, inferSchema=True)\n",
    "contract_2014_02_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_02.csv\", header=True, inferSchema=True)\n",
    "contract_2014_03_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_03.csv\", header=True, inferSchema=True)\n",
    "contract_2014_04_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_04.csv\", header=True, inferSchema=True)\n",
    "contract_2014_05_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_05.csv\", header=True, inferSchema=True)\n",
    "contract_2014_06_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_06.csv\", header=True, inferSchema=True)\n",
    "contract_2014_07_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_07.csv\", header=True, inferSchema=True)\n",
    "contract_2014_08_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_08.csv\", header=True, inferSchema=True)\n",
    "contract_2014_09_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_09.csv\", header=True, inferSchema=True)\n",
    "contract_2014_10_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_10.csv\", header=True, inferSchema=True)\n",
    "contract_2014_11_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_11.csv\", header=True, inferSchema=True)\n",
    "contract_2014_12_df = spark.read.csv(\"./2014/CPSC_Contract_Info_2014_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_01_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_02_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_03_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_04_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_05_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_06_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_07_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_08_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_09_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_10_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_11_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2014_12_df = spark.read.csv(\"./2014/CPSC_Enrollment_Info_2014_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2014_01_df.printSchema()\n",
    "enrollment_2014_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2014 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2014_merged_df = contract_2014_01_df.join(enrollment_2014_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2014_merged_df = contract_2014_02_df.join(enrollment_2014_02_df, on='Plan ID', how='outer')\n",
    "Mar_2014_merged_df = contract_2014_03_df.join(enrollment_2014_03_df, on='Plan ID', how='outer')\n",
    "Apr_2014_merged_df = contract_2014_04_df.join(enrollment_2014_04_df, on='Plan ID', how='outer')\n",
    "May_2014_merged_df = contract_2014_05_df.join(enrollment_2014_05_df, on='Plan ID', how='outer')\n",
    "Jun_2014_merged_df = contract_2014_06_df.join(enrollment_2014_06_df, on='Plan ID', how='outer')\n",
    "Jul_2014_merged_df = contract_2014_07_df.join(enrollment_2014_07_df, on='Plan ID', how='outer')\n",
    "Aug_2014_merged_df = contract_2014_08_df.join(enrollment_2014_08_df, on='Plan ID', how='outer')\n",
    "Sep_2014_merged_df = contract_2014_09_df.join(enrollment_2014_09_df, on='Plan ID', how='outer')\n",
    "Oct_2014_merged_df = contract_2014_10_df.join(enrollment_2014_10_df, on='Plan ID', how='outer')\n",
    "Nov_2014_merged_df = contract_2014_11_df.join(enrollment_2014_11_df, on='Plan ID', how='outer')\n",
    "Dec_2014_merged_df = contract_2014_12_df.join(enrollment_2014_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2014_merged_df = Jan_2014_merged_df.drop(*columns_to_drop)\n",
    "Feb_2014_merged_df = Feb_2014_merged_df.drop(*columns_to_drop)\n",
    "Mar_2014_merged_df = Mar_2014_merged_df.drop(*columns_to_drop)\n",
    "Apr_2014_merged_df = Apr_2014_merged_df.drop(*columns_to_drop)\n",
    "May_2014_merged_df = May_2014_merged_df.drop(*columns_to_drop)\n",
    "Jun_2014_merged_df = Jun_2014_merged_df.drop(*columns_to_drop)\n",
    "Jul_2014_merged_df = Jul_2014_merged_df.drop(*columns_to_drop)\n",
    "Aug_2014_merged_df = Aug_2014_merged_df.drop(*columns_to_drop)\n",
    "Sep_2014_merged_df = Sep_2014_merged_df.drop(*columns_to_drop)\n",
    "Oct_2014_merged_df = Oct_2014_merged_df.drop(*columns_to_drop)\n",
    "Nov_2014_merged_df = Nov_2014_merged_df.drop(*columns_to_drop)\n",
    "Dec_2014_merged_df = Dec_2014_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2014_dfs = {\n",
    "    'Jan_2014': (Jan_2014_merged_df, 'January', '2014')\n",
    "    'Feb_2014': (Feb_2014_merged_df, 'February', '2014'),\n",
    "    'Mar_2014': (Mar_2014_merged_df, 'March', '2014'),\n",
    "    'Apr_2014': (Apr_2014_merged_df, 'April', '2014'),\n",
    "    'May_2014': (May_2014_merged_df, 'May', '2014'),\n",
    "    'Jun_2014': (Jun_2014_merged_df, 'June', '2014'),\n",
    "    'Jul_2014': (Jul_2014_merged_df, 'July', '2014'),\n",
    "    'Aug_2014': (Aug_2014_merged_df, 'August', '2014'),\n",
    "    'Sep_2014': (Sep_2014_merged_df, 'September', '2014'),\n",
    "    'Oct_2014': (Oct_2014_merged_df, 'October', '2014'),\n",
    "    'Nov_2014': (Nov_2014_merged_df, 'November', '2014'),\n",
    "    'Dec_2014': (Dec_2014_merged_df, 'December', '2014')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2014_dfs:\n",
    "    df, month, year = merged_2014_dfs[values]\n",
    "    merged_2014_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2014_merged_df = merged_2014_dfs['Jan_2014']\n",
    "Feb_2014_merged_df = merged_2014_dfs['Feb_2014']\n",
    "Mar_2014_merged_df = merged_2014_dfs['Mar_2014']\n",
    "Apr_2014_merged_df = merged_2014_dfs['Apr_2014']\n",
    "May_2014_merged_df = merged_2014_dfs['May_2014']\n",
    "Jun_2014_merged_df = merged_2014_dfs['Jun_2014']\n",
    "Jul_2014_merged_df = merged_2014_dfs['Jul_2014']\n",
    "Aug_2014_merged_df = merged_2014_dfs['Aug_2014']\n",
    "Sep_2014_merged_df = merged_2014_dfs['Sep_2014']\n",
    "Oct_2014_merged_df = merged_2014_dfs['Oct_2014']\n",
    "Nov_2014_merged_df = merged_2014_dfs['Nov_2014']\n",
    "Dec_2014_merged_df = merged_2014_dfs['Dec_2014']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2014_dfs = {\n",
    "    'Jan_2014': Jan_2014_merged_df\n",
    "    'Feb_2014': Feb_2014_merged_df,\n",
    "    'Mar_2014': Mar_2014_merged_df,\n",
    "    'Apr_2014': Apr_2014_merged_df,\n",
    "    'May_2014': May_2014_merged_df,\n",
    "    'Jun_2014': Jun_2014_merged_df,\n",
    "    'Jul_2014': Jul_2014_merged_df,\n",
    "    'Aug_2014': Aug_2014_merged_df,\n",
    "    'Sep_2014': Sep_2014_merged_df,\n",
    "    'Oct_2014': Oct_2014_merged_df,\n",
    "    'Nov_2014': Nov_2014_merged_df,\n",
    "    'Dec_2014': Dec_2014_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2014_dfs:\n",
    "    merged_2014_dfs[values] = add_total_enrollments(merged_2014_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2014_merged_df = merged_2014_dfs['Jan_2014']\n",
    "Feb_2014_merged_df = merged_2014_dfs['Feb_2014']\n",
    "Mar_2014_merged_df = merged_2014_dfs['Mar_2014']\n",
    "Apr_2014_merged_df = merged_2014_dfs['Apr_2014']\n",
    "May_2014_merged_df = merged_2014_dfs['May_2014']\n",
    "Jun_2014_merged_df = merged_2014_dfs['Jun_2014']\n",
    "Jul_2014_merged_df = merged_2014_dfs['Jul_2014']\n",
    "Aug_2014_merged_df = merged_2014_dfs['Aug_2014']\n",
    "Sep_2014_merged_df = merged_2014_dfs['Sep_2014']\n",
    "Oct_2014_merged_df = merged_2014_dfs['Oct_2014']\n",
    "Nov_2014_merged_df = merged_2014_dfs['Nov_2014']\n",
    "Dec_2014_merged_df = merged_2014_dfs['Dec_2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2014_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2014 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2014': Jan_2014_merged_df\n",
    "    'Feb_2014': Feb_2014_merged_df,\n",
    "    'Mar_2014': Mar_2014_merged_df,\n",
    "    'Apr_2014': Apr_2014_merged_df,\n",
    "    'May_2014': May_2014_merged_df,\n",
    "    'Jun_2014': Jun_2014_merged_df,\n",
    "    'Jul_2014': Jul_2014_merged_df,\n",
    "    'Aug_2014': Aug_2014_merged_df,\n",
    "    'Sep_2014': Sep_2014_merged_df,\n",
    "    'Oct_2014': Oct_2014_merged_df,\n",
    "    'Nov_2014': Nov_2014_merged_df,\n",
    "    'Dec_2014': Dec_2014_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2015 CSV files into DataFrames\n",
    "contract_2015_01_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_01.csv\", header=True, inferSchema=True)\n",
    "contract_2015_02_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_02.csv\", header=True, inferSchema=True)\n",
    "contract_2015_03_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_03.csv\", header=True, inferSchema=True)\n",
    "contract_2015_04_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_04.csv\", header=True, inferSchema=True)\n",
    "contract_2015_05_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_05.csv\", header=True, inferSchema=True)\n",
    "contract_2015_06_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_06.csv\", header=True, inferSchema=True)\n",
    "contract_2015_07_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_07.csv\", header=True, inferSchema=True)\n",
    "contract_2015_08_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_08.csv\", header=True, inferSchema=True)\n",
    "contract_2015_09_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_09.csv\", header=True, inferSchema=True)\n",
    "contract_2015_10_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_10.csv\", header=True, inferSchema=True)\n",
    "contract_2015_11_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_11.csv\", header=True, inferSchema=True)\n",
    "contract_2015_12_df = spark.read.csv(\"./2015/CPSC_Contract_Info_2015_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_01_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_02_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_03_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_04_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_05_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_06_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_07_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_08_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_09_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_10_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_11_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2015_12_df = spark.read.csv(\"./2015/CPSC_Enrollment_Info_2015_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2015_01_df.printSchema()\n",
    "enrollment_2015_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2015 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2015_merged_df = contract_2015_01_df.join(enrollment_2015_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2015_merged_df = contract_2015_02_df.join(enrollment_2015_02_df, on='Plan ID', how='outer')\n",
    "Mar_2015_merged_df = contract_2015_03_df.join(enrollment_2015_03_df, on='Plan ID', how='outer')\n",
    "Apr_2015_merged_df = contract_2015_04_df.join(enrollment_2015_04_df, on='Plan ID', how='outer')\n",
    "May_2015_merged_df = contract_2015_05_df.join(enrollment_2015_05_df, on='Plan ID', how='outer')\n",
    "Jun_2015_merged_df = contract_2015_06_df.join(enrollment_2015_06_df, on='Plan ID', how='outer')\n",
    "Jul_2015_merged_df = contract_2015_07_df.join(enrollment_2015_07_df, on='Plan ID', how='outer')\n",
    "Aug_2015_merged_df = contract_2015_08_df.join(enrollment_2015_08_df, on='Plan ID', how='outer')\n",
    "Sep_2015_merged_df = contract_2015_09_df.join(enrollment_2015_09_df, on='Plan ID', how='outer')\n",
    "Oct_2015_merged_df = contract_2015_10_df.join(enrollment_2015_10_df, on='Plan ID', how='outer')\n",
    "Nov_2015_merged_df = contract_2015_11_df.join(enrollment_2015_11_df, on='Plan ID', how='outer')\n",
    "Dec_2015_merged_df = contract_2015_12_df.join(enrollment_2015_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2015_merged_df = Jan_2015_merged_df.drop(*columns_to_drop)\n",
    "Feb_2015_merged_df = Feb_2015_merged_df.drop(*columns_to_drop)\n",
    "Mar_2015_merged_df = Mar_2015_merged_df.drop(*columns_to_drop)\n",
    "Apr_2015_merged_df = Apr_2015_merged_df.drop(*columns_to_drop)\n",
    "May_2015_merged_df = May_2015_merged_df.drop(*columns_to_drop)\n",
    "Jun_2015_merged_df = Jun_2015_merged_df.drop(*columns_to_drop)\n",
    "Jul_2015_merged_df = Jul_2015_merged_df.drop(*columns_to_drop)\n",
    "Aug_2015_merged_df = Aug_2015_merged_df.drop(*columns_to_drop)\n",
    "Sep_2015_merged_df = Sep_2015_merged_df.drop(*columns_to_drop)\n",
    "Oct_2015_merged_df = Oct_2015_merged_df.drop(*columns_to_drop)\n",
    "Nov_2015_merged_df = Nov_2015_merged_df.drop(*columns_to_drop)\n",
    "Dec_2015_merged_df = Dec_2015_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2015_dfs = {\n",
    "    'Jan_2015': (Jan_2015_merged_df, 'January', '2015')\n",
    "    'Feb_2015': (Feb_2015_merged_df, 'February', '2015'),\n",
    "    'Mar_2015': (Mar_2015_merged_df, 'March', '2015'),\n",
    "    'Apr_2015': (Apr_2015_merged_df, 'April', '2015'),\n",
    "    'May_2015': (May_2015_merged_df, 'May', '2015'),\n",
    "    'Jun_2015': (Jun_2015_merged_df, 'June', '2015'),\n",
    "    'Jul_2015': (Jul_2015_merged_df, 'July', '2015'),\n",
    "    'Aug_2015': (Aug_2015_merged_df, 'August', '2015'),\n",
    "    'Sep_2015': (Sep_2015_merged_df, 'September', '2015'),\n",
    "    'Oct_2015': (Oct_2015_merged_df, 'October', '2015'),\n",
    "    'Nov_2015': (Nov_2015_merged_df, 'November', '2015'),\n",
    "    'Dec_2015': (Dec_2015_merged_df, 'December', '2015')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2015_dfs:\n",
    "    df, month, year = merged_2015_dfs[values]\n",
    "    merged_2015_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2015_merged_df = merged_2015_dfs['Jan_2015']\n",
    "Feb_2015_merged_df = merged_2015_dfs['Feb_2015']\n",
    "Mar_2015_merged_df = merged_2015_dfs['Mar_2015']\n",
    "Apr_2015_merged_df = merged_2015_dfs['Apr_2015']\n",
    "May_2015_merged_df = merged_2015_dfs['May_2015']\n",
    "Jun_2015_merged_df = merged_2015_dfs['Jun_2015']\n",
    "Jul_2015_merged_df = merged_2015_dfs['Jul_2015']\n",
    "Aug_2015_merged_df = merged_2015_dfs['Aug_2015']\n",
    "Sep_2015_merged_df = merged_2015_dfs['Sep_2015']\n",
    "Oct_2015_merged_df = merged_2015_dfs['Oct_2015']\n",
    "Nov_2015_merged_df = merged_2015_dfs['Nov_2015']\n",
    "Dec_2015_merged_df = merged_2015_dfs['Dec_2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2015_dfs = {\n",
    "    'Jan_2015': Jan_2015_merged_df\n",
    "    'Feb_2015': Feb_2015_merged_df,\n",
    "    'Mar_2015': Mar_2015_merged_df,\n",
    "    'Apr_2015': Apr_2015_merged_df,\n",
    "    'May_2015': May_2015_merged_df,\n",
    "    'Jun_2015': Jun_2015_merged_df,\n",
    "    'Jul_2015': Jul_2015_merged_df,\n",
    "    'Aug_2015': Aug_2015_merged_df,\n",
    "    'Sep_2015': Sep_2015_merged_df,\n",
    "    'Oct_2015': Oct_2015_merged_df,\n",
    "    'Nov_2015': Nov_2015_merged_df,\n",
    "    'Dec_2015': Dec_2015_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2015_dfs:\n",
    "    merged_2015_dfs[values] = add_total_enrollments(merged_2015_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2015_merged_df = merged_2015_dfs['Jan_2015']\n",
    "Feb_2015_merged_df = merged_2015_dfs['Feb_2015']\n",
    "Mar_2015_merged_df = merged_2015_dfs['Mar_2015']\n",
    "Apr_2015_merged_df = merged_2015_dfs['Apr_2015']\n",
    "May_2015_merged_df = merged_2015_dfs['May_2015']\n",
    "Jun_2015_merged_df = merged_2015_dfs['Jun_2015']\n",
    "Jul_2015_merged_df = merged_2015_dfs['Jul_2015']\n",
    "Aug_2015_merged_df = merged_2015_dfs['Aug_2015']\n",
    "Sep_2015_merged_df = merged_2015_dfs['Sep_2015']\n",
    "Oct_2015_merged_df = merged_2015_dfs['Oct_2015']\n",
    "Nov_2015_merged_df = merged_2015_dfs['Nov_2015']\n",
    "Dec_2015_merged_df = merged_2015_dfs['Dec_2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2015_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2015 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2015': Jan_2015_merged_df\n",
    "    'Feb_2015': Feb_2015_merged_df,\n",
    "    'Mar_2015': Mar_2015_merged_df,\n",
    "    'Apr_2015': Apr_2015_merged_df,\n",
    "    'May_2015': May_2015_merged_df,\n",
    "    'Jun_2015': Jun_2015_merged_df,\n",
    "    'Jul_2015': Jul_2015_merged_df,\n",
    "    'Aug_2015': Aug_2015_merged_df,\n",
    "    'Sep_2015': Sep_2015_merged_df,\n",
    "    'Oct_2015': Oct_2015_merged_df,\n",
    "    'Nov_2015': Nov_2015_merged_df,\n",
    "    'Dec_2015': Dec_2015_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2016 CSV files into DataFrames\n",
    "contract_2016_01_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_01.csv\", header=True, inferSchema=True)\n",
    "contract_2016_02_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_02.csv\", header=True, inferSchema=True)\n",
    "contract_2016_03_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_03.csv\", header=True, inferSchema=True)\n",
    "contract_2016_04_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_04.csv\", header=True, inferSchema=True)\n",
    "contract_2016_05_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_05.csv\", header=True, inferSchema=True)\n",
    "contract_2016_06_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_06.csv\", header=True, inferSchema=True)\n",
    "contract_2016_07_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_07.csv\", header=True, inferSchema=True)\n",
    "contract_2016_08_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_08.csv\", header=True, inferSchema=True)\n",
    "contract_2016_09_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_09.csv\", header=True, inferSchema=True)\n",
    "contract_2016_10_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_10.csv\", header=True, inferSchema=True)\n",
    "contract_2016_11_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_11.csv\", header=True, inferSchema=True)\n",
    "contract_2016_12_df = spark.read.csv(\"./2016/CPSC_Contract_Info_2016_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_01_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_02_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_03_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_04_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_05_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_06_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_07_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_08_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_09_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_10_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_11_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2016_12_df = spark.read.csv(\"./2016/CPSC_Enrollment_Info_2016_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2016_01_df.printSchema()\n",
    "enrollment_2016_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2016 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2016_merged_df = contract_2016_01_df.join(enrollment_2016_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2016_merged_df = contract_2016_02_df.join(enrollment_2016_02_df, on='Plan ID', how='outer')\n",
    "Mar_2016_merged_df = contract_2016_03_df.join(enrollment_2016_03_df, on='Plan ID', how='outer')\n",
    "Apr_2016_merged_df = contract_2016_04_df.join(enrollment_2016_04_df, on='Plan ID', how='outer')\n",
    "May_2016_merged_df = contract_2016_05_df.join(enrollment_2016_05_df, on='Plan ID', how='outer')\n",
    "Jun_2016_merged_df = contract_2016_06_df.join(enrollment_2016_06_df, on='Plan ID', how='outer')\n",
    "Jul_2016_merged_df = contract_2016_07_df.join(enrollment_2016_07_df, on='Plan ID', how='outer')\n",
    "Aug_2016_merged_df = contract_2016_08_df.join(enrollment_2016_08_df, on='Plan ID', how='outer')\n",
    "Sep_2016_merged_df = contract_2016_09_df.join(enrollment_2016_09_df, on='Plan ID', how='outer')\n",
    "Oct_2016_merged_df = contract_2016_10_df.join(enrollment_2016_10_df, on='Plan ID', how='outer')\n",
    "Nov_2016_merged_df = contract_2016_11_df.join(enrollment_2016_11_df, on='Plan ID', how='outer')\n",
    "Dec_2016_merged_df = contract_2016_12_df.join(enrollment_2016_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2016_merged_df = Jan_2016_merged_df.drop(*columns_to_drop)\n",
    "Feb_2016_merged_df = Feb_2016_merged_df.drop(*columns_to_drop)\n",
    "Mar_2016_merged_df = Mar_2016_merged_df.drop(*columns_to_drop)\n",
    "Apr_2016_merged_df = Apr_2016_merged_df.drop(*columns_to_drop)\n",
    "May_2016_merged_df = May_2016_merged_df.drop(*columns_to_drop)\n",
    "Jun_2016_merged_df = Jun_2016_merged_df.drop(*columns_to_drop)\n",
    "Jul_2016_merged_df = Jul_2016_merged_df.drop(*columns_to_drop)\n",
    "Aug_2016_merged_df = Aug_2016_merged_df.drop(*columns_to_drop)\n",
    "Sep_2016_merged_df = Sep_2016_merged_df.drop(*columns_to_drop)\n",
    "Oct_2016_merged_df = Oct_2016_merged_df.drop(*columns_to_drop)\n",
    "Nov_2016_merged_df = Nov_2016_merged_df.drop(*columns_to_drop)\n",
    "Dec_2016_merged_df = Dec_2016_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2016_dfs = {\n",
    "    'Jan_2016': (Jan_2016_merged_df, 'January', '2016')\n",
    "    'Feb_2016': (Feb_2016_merged_df, 'February', '2016'),\n",
    "    'Mar_2016': (Mar_2016_merged_df, 'March', '2016'),\n",
    "    'Apr_2016': (Apr_2016_merged_df, 'April', '2016'),\n",
    "    'May_2016': (May_2016_merged_df, 'May', '2016'),\n",
    "    'Jun_2016': (Jun_2016_merged_df, 'June', '2016'),\n",
    "    'Jul_2016': (Jul_2016_merged_df, 'July', '2016'),\n",
    "    'Aug_2016': (Aug_2016_merged_df, 'August', '2016'),\n",
    "    'Sep_2016': (Sep_2016_merged_df, 'September', '2016'),\n",
    "    'Oct_2016': (Oct_2016_merged_df, 'October', '2016'),\n",
    "    'Nov_2016': (Nov_2016_merged_df, 'November', '2016'),\n",
    "    'Dec_2016': (Dec_2016_merged_df, 'December', '2016')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2016_dfs:\n",
    "    df, month, year = merged_2016_dfs[values]\n",
    "    merged_2016_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2016_merged_df = merged_2016_dfs['Jan_2016']\n",
    "Feb_2016_merged_df = merged_2016_dfs['Feb_2016']\n",
    "Mar_2016_merged_df = merged_2016_dfs['Mar_2016']\n",
    "Apr_2016_merged_df = merged_2016_dfs['Apr_2016']\n",
    "May_2016_merged_df = merged_2016_dfs['May_2016']\n",
    "Jun_2016_merged_df = merged_2016_dfs['Jun_2016']\n",
    "Jul_2016_merged_df = merged_2016_dfs['Jul_2016']\n",
    "Aug_2016_merged_df = merged_2016_dfs['Aug_2016']\n",
    "Sep_2016_merged_df = merged_2016_dfs['Sep_2016']\n",
    "Oct_2016_merged_df = merged_2016_dfs['Oct_2016']\n",
    "Nov_2016_merged_df = merged_2016_dfs['Nov_2016']\n",
    "Dec_2016_merged_df = merged_2016_dfs['Dec_2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2016_dfs = {\n",
    "    'Jan_2016': Jan_2016_merged_df\n",
    "    'Feb_2016': Feb_2016_merged_df,\n",
    "    'Mar_2016': Mar_2016_merged_df,\n",
    "    'Apr_2016': Apr_2016_merged_df,\n",
    "    'May_2016': May_2016_merged_df,\n",
    "    'Jun_2016': Jun_2016_merged_df,\n",
    "    'Jul_2016': Jul_2016_merged_df,\n",
    "    'Aug_2016': Aug_2016_merged_df,\n",
    "    'Sep_2016': Sep_2016_merged_df,\n",
    "    'Oct_2016': Oct_2016_merged_df,\n",
    "    'Nov_2016': Nov_2016_merged_df,\n",
    "    'Dec_2016': Dec_2016_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2016_dfs:\n",
    "    merged_2016_dfs[values] = add_total_enrollments(merged_2016_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2016_merged_df = merged_2016_dfs['Jan_2016']\n",
    "Feb_2016_merged_df = merged_2016_dfs['Feb_2016']\n",
    "Mar_2016_merged_df = merged_2016_dfs['Mar_2016']\n",
    "Apr_2016_merged_df = merged_2016_dfs['Apr_2016']\n",
    "May_2016_merged_df = merged_2016_dfs['May_2016']\n",
    "Jun_2016_merged_df = merged_2016_dfs['Jun_2016']\n",
    "Jul_2016_merged_df = merged_2016_dfs['Jul_2016']\n",
    "Aug_2016_merged_df = merged_2016_dfs['Aug_2016']\n",
    "Sep_2016_merged_df = merged_2016_dfs['Sep_2016']\n",
    "Oct_2016_merged_df = merged_2016_dfs['Oct_2016']\n",
    "Nov_2016_merged_df = merged_2016_dfs['Nov_2016']\n",
    "Dec_2016_merged_df = merged_2016_dfs['Dec_2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2016_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2016 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2016': Jan_2016_merged_df\n",
    "    'Feb_2016': Feb_2016_merged_df,\n",
    "    'Mar_2016': Mar_2016_merged_df,\n",
    "    'Apr_2016': Apr_2016_merged_df,\n",
    "    'May_2016': May_2016_merged_df,\n",
    "    'Jun_2016': Jun_2016_merged_df,\n",
    "    'Jul_2016': Jul_2016_merged_df,\n",
    "    'Aug_2016': Aug_2016_merged_df,\n",
    "    'Sep_2016': Sep_2016_merged_df,\n",
    "    'Oct_2016': Oct_2016_merged_df,\n",
    "    'Nov_2016': Nov_2016_merged_df,\n",
    "    'Dec_2016': Dec_2016_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2017 CSV files into DataFrames\n",
    "contract_2017_01_df = spark.read.csv(\"./2017/CPSC_Contract_Info_2017_01.csv\", header=True, inferSchema=True)\n",
    "contract_2017_02_df = spark.read.csv(\"./2017/CPSC_Contract_Info_2017_02.csv\", header=True, inferSchema=True)\n",
    "contract_2017_07_df = spark.read.csv(\"./2017/CPSC_Contract_Info_2017_07.csv\", header=True, inferSchema=True)\n",
    "contract_2017_08_df = spark.read.csv(\"./2017/CPSC_Contract_Info_2017_08.csv\", header=True, inferSchema=True)\n",
    "contract_2017_09_df = spark.read.csv(\"./2017/CPSC_Contract_Info_2017_09.csv\", header=True, inferSchema=True)\n",
    "contract_2017_10_df = spark.read.csv(\"./2017/CPSC_Contract_Info_2017_10.csv\", header=True, inferSchema=True)\n",
    "contract_2017_12_df = spark.read.csv(\"./2017/CPSC_Contract_Info_2017_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2017_01_df = spark.read.csv(\"./2017/CPSC_Enrollment_Info_2017_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2017_02_df = spark.read.csv(\"./2017/CPSC_Enrollment_Info_2017_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2017_07_df = spark.read.csv(\"./2017/CPSC_Enrollment_Info_2017_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2017_08_df = spark.read.csv(\"./2017/CPSC_Enrollment_Info_2017_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2017_09_df = spark.read.csv(\"./2017/CPSC_Enrollment_Info_2017_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2017_10_df = spark.read.csv(\"./2017/CPSC_Enrollment_Info_2017_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2017_12_df = spark.read.csv(\"./2017/CPSC_Enrollment_Info_2017_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2017_01_df.printSchema()\n",
    "enrollment_2017_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2017 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2017_merged_df = contract_2017_01_df.join(enrollment_2017_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2017_merged_df = contract_2017_02_df.join(enrollment_2017_02_df, on='Plan ID', how='outer')\n",
    "Jul_2017_merged_df = contract_2017_07_df.join(enrollment_2017_07_df, on='Plan ID', how='outer')\n",
    "Aug_2017_merged_df = contract_2017_08_df.join(enrollment_2017_08_df, on='Plan ID', how='outer')\n",
    "Sep_2017_merged_df = contract_2017_09_df.join(enrollment_2017_09_df, on='Plan ID', how='outer')\n",
    "Oct_2017_merged_df = contract_2017_10_df.join(enrollment_2017_10_df, on='Plan ID', how='outer')\n",
    "Dec_2017_merged_df = contract_2017_12_df.join(enrollment_2017_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2017_merged_df = Jan_2017_merged_df.drop(*columns_to_drop)\n",
    "Feb_2017_merged_df = Feb_2017_merged_df.drop(*columns_to_drop)\n",
    "Jul_2017_merged_df = Jul_2017_merged_df.drop(*columns_to_drop)\n",
    "Aug_2017_merged_df = Aug_2017_merged_df.drop(*columns_to_drop)\n",
    "Sep_2017_merged_df = Sep_2017_merged_df.drop(*columns_to_drop)\n",
    "Oct_2017_merged_df = Oct_2017_merged_df.drop(*columns_to_drop)\n",
    "Dec_2017_merged_df = Dec_2017_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2017_dfs = {\n",
    "    'Jan_2017': (Jan_2017_merged_df, 'January', '2017')\n",
    "    'Feb_2017': (Feb_2017_merged_df, 'February', '2017'),\n",
    "    'Jul_2017': (Jul_2017_merged_df, 'July', '2017'),\n",
    "    'Aug_2017': (Aug_2017_merged_df, 'August', '2017'),\n",
    "    'Sep_2017': (Sep_2017_merged_df, 'September', '2017'),\n",
    "    'Oct_2017': (Oct_2017_merged_df, 'October', '2017'),\n",
    "    'Dec_2017': (Dec_2017_merged_df, 'December', '2017')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2017_dfs:\n",
    "    df, month, year = merged_2017_dfs[values]\n",
    "    merged_2017_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2017_merged_df = merged_2017_dfs['Jan_2017']\n",
    "Feb_2017_merged_df = merged_2017_dfs['Feb_2017']\n",
    "Jul_2017_merged_df = merged_2017_dfs['Jul_2017']\n",
    "Aug_2017_merged_df = merged_2017_dfs['Aug_2017']\n",
    "Sep_2017_merged_df = merged_2017_dfs['Sep_2017']\n",
    "Oct_2017_merged_df = merged_2017_dfs['Oct_2017']\n",
    "Dec_2017_merged_df = merged_2017_dfs['Dec_2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2017_dfs = {\n",
    "    'Jan_2017': Jan_2017_merged_df\n",
    "    'Feb_2017': Feb_2017_merged_df,\n",
    "    'Jul_2017': Jul_2017_merged_df,\n",
    "    'Aug_2017': Aug_2017_merged_df,\n",
    "    'Sep_2017': Sep_2017_merged_df,\n",
    "    'Oct_2017': Oct_2017_merged_df,\n",
    "    'Dec_2017': Dec_2017_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2017_dfs:\n",
    "    merged_2017_dfs[values] = add_total_enrollments(merged_2017_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2017_merged_df = merged_2017_dfs['Jan_2017']\n",
    "Feb_2017_merged_df = merged_2017_dfs['Feb_2017']\n",
    "Jul_2017_merged_df = merged_2017_dfs['Jul_2017']\n",
    "Aug_2017_merged_df = merged_2017_dfs['Aug_2017']\n",
    "Sep_2017_merged_df = merged_2017_dfs['Sep_2017']\n",
    "Oct_2017_merged_df = merged_2017_dfs['Oct_2017']\n",
    "Dec_2017_merged_df = merged_2017_dfs['Dec_2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Feb_2017_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2017 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2017': Jan_2017_merged_df\n",
    "    'Feb_2017': Feb_2017_merged_df,\n",
    "    'Mar_2017': Mar_2017_merged_df,\n",
    "    'Apr_2017': Apr_2017_merged_df,\n",
    "    'May_2017': May_2017_merged_df,\n",
    "    'Jun_2017': Jun_2017_merged_df,\n",
    "    'Jul_2017': Jul_2017_merged_df,\n",
    "    'Aug_2017': Aug_2017_merged_df,\n",
    "    'Sep_2017': Sep_2017_merged_df,\n",
    "    'Oct_2017': Oct_2017_merged_df,\n",
    "    'Nov_2017': Nov_2017_merged_df,\n",
    "    'Dec_2017': Dec_2017_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2018 CSV files into DataFrames\n",
    "contract_2018_01_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_01.csv\", header=True, inferSchema=True)\n",
    "contract_2018_02_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_02.csv\", header=True, inferSchema=True)\n",
    "contract_2018_03_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_03.csv\", header=True, inferSchema=True)\n",
    "contract_2018_04_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_04.csv\", header=True, inferSchema=True)\n",
    "contract_2018_05_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_05.csv\", header=True, inferSchema=True)\n",
    "contract_2018_06_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_06.csv\", header=True, inferSchema=True)\n",
    "contract_2018_07_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_07.csv\", header=True, inferSchema=True)\n",
    "#contract_2018_08_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_08.csv\", header=True, inferSchema=True)\n",
    "contract_2018_09_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_09.csv\", header=True, inferSchema=True)\n",
    "contract_2018_10_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_10.csv\", header=True, inferSchema=True)\n",
    "contract_2018_11_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_11.csv\", header=True, inferSchema=True)\n",
    "contract_2018_12_df = spark.read.csv(\"./2018/CPSC_Contract_Info_2018_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_01_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_02_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_03_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_04_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_05_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_06_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_07_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_07.csv\", header=True, inferSchema=True)\n",
    "#enrollment_2018_08_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_09_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_10_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_11_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2018_12_df = spark.read.csv(\"./2018/CPSC_Enrollment_Info_2018_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2018_01_df.printSchema()\n",
    "enrollment_2018_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2018 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2018_merged_df = contract_2018_01_df.join(enrollment_2018_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2018_merged_df = contract_2018_02_df.join(enrollment_2018_02_df, on='Plan ID', how='outer')\n",
    "Mar_2018_merged_df = contract_2018_03_df.join(enrollment_2018_03_df, on='Plan ID', how='outer')\n",
    "Apr_2018_merged_df = contract_2018_04_df.join(enrollment_2018_04_df, on='Plan ID', how='outer')\n",
    "May_2018_merged_df = contract_2018_05_df.join(enrollment_2018_05_df, on='Plan ID', how='outer')\n",
    "Jun_2018_merged_df = contract_2018_06_df.join(enrollment_2018_06_df, on='Plan ID', how='outer')\n",
    "Jul_2018_merged_df = contract_2018_07_df.join(enrollment_2018_07_df, on='Plan ID', how='outer')\n",
    "Sep_2018_merged_df = contract_2018_09_df.join(enrollment_2018_09_df, on='Plan ID', how='outer')\n",
    "Oct_2018_merged_df = contract_2018_10_df.join(enrollment_2018_10_df, on='Plan ID', how='outer')\n",
    "Nov_2018_merged_df = contract_2018_11_df.join(enrollment_2018_11_df, on='Plan ID', how='outer')\n",
    "Dec_2018_merged_df = contract_2018_12_df.join(enrollment_2018_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2018_merged_df = Jan_2018_merged_df.drop(*columns_to_drop)\n",
    "Feb_2018_merged_df = Feb_2018_merged_df.drop(*columns_to_drop)\n",
    "Mar_2018_merged_df = Mar_2018_merged_df.drop(*columns_to_drop)\n",
    "Apr_2018_merged_df = Apr_2018_merged_df.drop(*columns_to_drop)\n",
    "May_2018_merged_df = May_2018_merged_df.drop(*columns_to_drop)\n",
    "Jun_2018_merged_df = Jun_2018_merged_df.drop(*columns_to_drop)\n",
    "Jul_2018_merged_df = Jul_2018_merged_df.drop(*columns_to_drop)\n",
    "Sep_2018_merged_df = Sep_2018_merged_df.drop(*columns_to_drop)\n",
    "Oct_2018_merged_df = Oct_2018_merged_df.drop(*columns_to_drop)\n",
    "Nov_2018_merged_df = Nov_2018_merged_df.drop(*columns_to_drop)\n",
    "Dec_2018_merged_df = Dec_2018_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2018_dfs = {\n",
    "    'Jan_2018': (Jan_2018_merged_df, 'January', '2018')\n",
    "    'Feb_2018': (Feb_2018_merged_df, 'February', '2018'),\n",
    "    'Mar_2018': (Mar_2018_merged_df, 'March', '2018'),\n",
    "    'Apr_2018': (Apr_2018_merged_df, 'April', '2018'),\n",
    "    'May_2018': (May_2018_merged_df, 'May', '2018'),\n",
    "    'Jun_2018': (Jun_2018_merged_df, 'June', '2018'),\n",
    "    'Jul_2018': (Jul_2018_merged_df, 'July', '2018'),\n",
    "    'Sep_2018': (Sep_2018_merged_df, 'September', '2018'),\n",
    "    'Oct_2018': (Oct_2018_merged_df, 'October', '2018'),\n",
    "    'Nov_2018': (Nov_2018_merged_df, 'November', '2018'),\n",
    "    'Dec_2018': (Dec_2018_merged_df, 'December', '2018')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2018_dfs:\n",
    "    df, month, year = merged_2018_dfs[values]\n",
    "    merged_2018_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2018_merged_df = merged_2018_dfs['Jan_2018']\n",
    "Feb_2018_merged_df = merged_2018_dfs['Feb_2018']\n",
    "Mar_2018_merged_df = merged_2018_dfs['Mar_2018']\n",
    "Apr_2018_merged_df = merged_2018_dfs['Apr_2018']\n",
    "May_2018_merged_df = merged_2018_dfs['May_2018']\n",
    "Jun_2018_merged_df = merged_2018_dfs['Jun_2018']\n",
    "Jul_2018_merged_df = merged_2018_dfs['Jul_2018']\n",
    "Sep_2018_merged_df = merged_2018_dfs['Sep_2018']\n",
    "Oct_2018_merged_df = merged_2018_dfs['Oct_2018']\n",
    "Nov_2018_merged_df = merged_2018_dfs['Nov_2018']\n",
    "Dec_2018_merged_df = merged_2018_dfs['Dec_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2018_dfs = {\n",
    "    'Jan_2018': Jan_2018_merged_df\n",
    "    'Feb_2018': Feb_2018_merged_df,\n",
    "    'Mar_2018': Mar_2018_merged_df,\n",
    "    'Apr_2018': Apr_2018_merged_df,\n",
    "    'May_2018': May_2018_merged_df,\n",
    "    'Jun_2018': Jun_2018_merged_df,\n",
    "    'Jul_2018': Jul_2018_merged_df,\n",
    "    'Sep_2018': Sep_2018_merged_df,\n",
    "    'Oct_2018': Oct_2018_merged_df,\n",
    "    'Nov_2018': Nov_2018_merged_df,\n",
    "    'Dec_2018': Dec_2018_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2018_dfs:\n",
    "    merged_2018_dfs[values] = add_total_enrollments(merged_2018_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2018_merged_df = merged_2018_dfs['Jan_2018']\n",
    "Feb_2018_merged_df = merged_2018_dfs['Feb_2018']\n",
    "Mar_2018_merged_df = merged_2018_dfs['Mar_2018']\n",
    "Apr_2018_merged_df = merged_2018_dfs['Apr_2018']\n",
    "May_2018_merged_df = merged_2018_dfs['May_2018']\n",
    "Jun_2018_merged_df = merged_2018_dfs['Jun_2018']\n",
    "Jul_2018_merged_df = merged_2018_dfs['Jul_2018']\n",
    "Sep_2018_merged_df = merged_2018_dfs['Sep_2018']\n",
    "Oct_2018_merged_df = merged_2018_dfs['Oct_2018']\n",
    "Nov_2018_merged_df = merged_2018_dfs['Nov_2018']\n",
    "Dec_2018_merged_df = merged_2018_dfs['Dec_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2018_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2018 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2018': Jan_2018_merged_df\n",
    "    'Feb_2018': Feb_2018_merged_df,\n",
    "    'Mar_2018': Mar_2018_merged_df,\n",
    "    'Apr_2018': Apr_2018_merged_df,\n",
    "    'May_2018': May_2018_merged_df,\n",
    "    'Jun_2018': Jun_2018_merged_df,\n",
    "    'Jul_2018': Jul_2018_merged_df,\n",
    "    'Aug_2018': Aug_2018_merged_df,\n",
    "    'Sep_2018': Sep_2018_merged_df,\n",
    "    'Oct_2018': Oct_2018_merged_df,\n",
    "    'Nov_2018': Nov_2018_merged_df,\n",
    "    'Dec_2018': Dec_2018_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2019 CSV files into DataFrames\n",
    "contract_2019_01_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_01.csv\", header=True, inferSchema=True)\n",
    "contract_2019_02_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_02.csv\", header=True, inferSchema=True)\n",
    "contract_2019_03_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_03.csv\", header=True, inferSchema=True)\n",
    "contract_2019_04_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_04.csv\", header=True, inferSchema=True)\n",
    "contract_2019_05_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_05.csv\", header=True, inferSchema=True)\n",
    "contract_2019_06_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_06.csv\", header=True, inferSchema=True)\n",
    "contract_2019_07_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_07.csv\", header=True, inferSchema=True)\n",
    "contract_2019_08_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_08.csv\", header=True, inferSchema=True)\n",
    "contract_2019_09_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_09.csv\", header=True, inferSchema=True)\n",
    "contract_2019_10_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_10.csv\", header=True, inferSchema=True)\n",
    "contract_2019_11_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_11.csv\", header=True, inferSchema=True)\n",
    "contract_2019_12_df = spark.read.csv(\"./2019/CPSC_Contract_Info_2019_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_01_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_02_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_03_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_04_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_05_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_06_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_07_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_08_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_09_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_10_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_11_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2019_12_df = spark.read.csv(\"./2019/CPSC_Enrollment_Info_2019_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2019_01_df.printSchema()\n",
    "enrollment_2019_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2019 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2019_merged_df = contract_2019_01_df.join(enrollment_2019_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2019_merged_df = contract_2019_02_df.join(enrollment_2019_02_df, on='Plan ID', how='outer')\n",
    "Mar_2019_merged_df = contract_2019_03_df.join(enrollment_2019_03_df, on='Plan ID', how='outer')\n",
    "Apr_2019_merged_df = contract_2019_04_df.join(enrollment_2019_04_df, on='Plan ID', how='outer')\n",
    "May_2019_merged_df = contract_2019_05_df.join(enrollment_2019_05_df, on='Plan ID', how='outer')\n",
    "Jun_2019_merged_df = contract_2019_06_df.join(enrollment_2019_06_df, on='Plan ID', how='outer')\n",
    "Jul_2019_merged_df = contract_2019_07_df.join(enrollment_2019_07_df, on='Plan ID', how='outer')\n",
    "Aug_2019_merged_df = contract_2019_08_df.join(enrollment_2019_08_df, on='Plan ID', how='outer')\n",
    "Sep_2019_merged_df = contract_2019_09_df.join(enrollment_2019_09_df, on='Plan ID', how='outer')\n",
    "Oct_2019_merged_df = contract_2019_10_df.join(enrollment_2019_10_df, on='Plan ID', how='outer')\n",
    "Nov_2019_merged_df = contract_2019_11_df.join(enrollment_2019_11_df, on='Plan ID', how='outer')\n",
    "Dec_2019_merged_df = contract_2019_12_df.join(enrollment_2019_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2019_merged_df = Jan_2019_merged_df.drop(*columns_to_drop)\n",
    "Feb_2019_merged_df = Feb_2019_merged_df.drop(*columns_to_drop)\n",
    "Mar_2019_merged_df = Mar_2019_merged_df.drop(*columns_to_drop)\n",
    "Apr_2019_merged_df = Apr_2019_merged_df.drop(*columns_to_drop)\n",
    "May_2019_merged_df = May_2019_merged_df.drop(*columns_to_drop)\n",
    "Jun_2019_merged_df = Jun_2019_merged_df.drop(*columns_to_drop)\n",
    "Jul_2019_merged_df = Jul_2019_merged_df.drop(*columns_to_drop)\n",
    "Aug_2019_merged_df = Aug_2019_merged_df.drop(*columns_to_drop)\n",
    "Sep_2019_merged_df = Sep_2019_merged_df.drop(*columns_to_drop)\n",
    "Oct_2019_merged_df = Oct_2019_merged_df.drop(*columns_to_drop)\n",
    "Nov_2019_merged_df = Nov_2019_merged_df.drop(*columns_to_drop)\n",
    "Dec_2019_merged_df = Dec_2019_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2019_dfs = {\n",
    "    'Jan_2019': (Jan_2019_merged_df, 'January', '2019')\n",
    "    'Feb_2019': (Feb_2019_merged_df, 'February', '2019'),\n",
    "    'Mar_2019': (Mar_2019_merged_df, 'March', '2019'),\n",
    "    'Apr_2019': (Apr_2019_merged_df, 'April', '2019'),\n",
    "    'May_2019': (May_2019_merged_df, 'May', '2019'),\n",
    "    'Jun_2019': (Jun_2019_merged_df, 'June', '2019'),\n",
    "    'Jul_2019': (Jul_2019_merged_df, 'July', '2019'),\n",
    "    'Aug_2019': (Aug_2019_merged_df, 'August', '2019'),\n",
    "    'Sep_2019': (Sep_2019_merged_df, 'September', '2019'),\n",
    "    'Oct_2019': (Oct_2019_merged_df, 'October', '2019'),\n",
    "    'Nov_2019': (Nov_2019_merged_df, 'November', '2019'),\n",
    "    'Dec_2019': (Dec_2019_merged_df, 'December', '2019')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2019_dfs:\n",
    "    df, month, year = merged_2019_dfs[values]\n",
    "    merged_2019_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2019_merged_df = merged_2019_dfs['Jan_2019']\n",
    "Feb_2019_merged_df = merged_2019_dfs['Feb_2019']\n",
    "Mar_2019_merged_df = merged_2019_dfs['Mar_2019']\n",
    "Apr_2019_merged_df = merged_2019_dfs['Apr_2019']\n",
    "May_2019_merged_df = merged_2019_dfs['May_2019']\n",
    "Jun_2019_merged_df = merged_2019_dfs['Jun_2019']\n",
    "Jul_2019_merged_df = merged_2019_dfs['Jul_2019']\n",
    "Aug_2019_merged_df = merged_2019_dfs['Aug_2019']\n",
    "Sep_2019_merged_df = merged_2019_dfs['Sep_2019']\n",
    "Oct_2019_merged_df = merged_2019_dfs['Oct_2019']\n",
    "Nov_2019_merged_df = merged_2019_dfs['Nov_2019']\n",
    "Dec_2019_merged_df = merged_2019_dfs['Dec_2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2019_dfs = {\n",
    "    'Jan_2019': Jan_2019_merged_df\n",
    "    'Feb_2019': Feb_2019_merged_df,\n",
    "    'Mar_2019': Mar_2019_merged_df,\n",
    "    'Apr_2019': Apr_2019_merged_df,\n",
    "    'May_2019': May_2019_merged_df,\n",
    "    'Jun_2019': Jun_2019_merged_df,\n",
    "    'Jul_2019': Jul_2019_merged_df,\n",
    "    'Aug_2019': Aug_2019_merged_df,\n",
    "    'Sep_2019': Sep_2019_merged_df,\n",
    "    'Oct_2019': Oct_2019_merged_df,\n",
    "    'Nov_2019': Nov_2019_merged_df,\n",
    "    'Dec_2019': Dec_2019_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2019_dfs:\n",
    "    merged_2019_dfs[values] = add_total_enrollments(merged_2019_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2019_merged_df = merged_2019_dfs['Jan_2019']\n",
    "Feb_2019_merged_df = merged_2019_dfs['Feb_2019']\n",
    "Mar_2019_merged_df = merged_2019_dfs['Mar_2019']\n",
    "Apr_2019_merged_df = merged_2019_dfs['Apr_2019']\n",
    "May_2019_merged_df = merged_2019_dfs['May_2019']\n",
    "Jun_2019_merged_df = merged_2019_dfs['Jun_2019']\n",
    "Jul_2019_merged_df = merged_2019_dfs['Jul_2019']\n",
    "Aug_2019_merged_df = merged_2019_dfs['Aug_2019']\n",
    "Sep_2019_merged_df = merged_2019_dfs['Sep_2019']\n",
    "Oct_2019_merged_df = merged_2019_dfs['Oct_2019']\n",
    "Nov_2019_merged_df = merged_2019_dfs['Nov_2019']\n",
    "Dec_2019_merged_df = merged_2019_dfs['Dec_2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2019_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2019 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2019': Jan_2019_merged_df\n",
    "    'Feb_2019': Feb_2019_merged_df,\n",
    "    'Mar_2019': Mar_2019_merged_df,\n",
    "    'Apr_2019': Apr_2019_merged_df,\n",
    "    'May_2019': May_2019_merged_df,\n",
    "    'Jun_2019': Jun_2019_merged_df,\n",
    "    'Jul_2019': Jul_2019_merged_df,\n",
    "    'Aug_2019': Aug_2019_merged_df,\n",
    "    'Sep_2019': Sep_2019_merged_df,\n",
    "    'Oct_2019': Oct_2019_merged_df,\n",
    "    'Nov_2019': Nov_2019_merged_df,\n",
    "    'Dec_2019': Dec_2019_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2020 CSV files into DataFrames\n",
    "contract_2020_01_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_01.csv\", header=True, inferSchema=True)\n",
    "contract_2020_02_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_02.csv\", header=True, inferSchema=True)\n",
    "contract_2020_03_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_03.csv\", header=True, inferSchema=True)\n",
    "contract_2020_04_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_04.csv\", header=True, inferSchema=True)\n",
    "contract_2020_05_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_05.csv\", header=True, inferSchema=True)\n",
    "contract_2020_06_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_06.csv\", header=True, inferSchema=True)\n",
    "contract_2020_07_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_07.csv\", header=True, inferSchema=True)\n",
    "contract_2020_08_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_08.csv\", header=True, inferSchema=True)\n",
    "contract_2020_09_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_09.csv\", header=True, inferSchema=True)\n",
    "contract_2020_10_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_10.csv\", header=True, inferSchema=True)\n",
    "contract_2020_11_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_11.csv\", header=True, inferSchema=True)\n",
    "contract_2020_12_df = spark.read.csv(\"./2020/CPSC_Contract_Info_2020_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_01_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_02_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_03_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_04_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_05_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_06_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_07_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_08_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_09_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_10_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_11_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2020_12_df = spark.read.csv(\"./2020/CPSC_Enrollment_Info_2020_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2020_01_df.printSchema()\n",
    "enrollment_2020_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2020 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2020_merged_df = contract_2020_01_df.join(enrollment_2020_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2020_merged_df = contract_2020_02_df.join(enrollment_2020_02_df, on='Plan ID', how='outer')\n",
    "Mar_2020_merged_df = contract_2020_03_df.join(enrollment_2020_03_df, on='Plan ID', how='outer')\n",
    "Apr_2020_merged_df = contract_2020_04_df.join(enrollment_2020_04_df, on='Plan ID', how='outer')\n",
    "May_2020_merged_df = contract_2020_05_df.join(enrollment_2020_05_df, on='Plan ID', how='outer')\n",
    "Jun_2020_merged_df = contract_2020_06_df.join(enrollment_2020_06_df, on='Plan ID', how='outer')\n",
    "Jul_2020_merged_df = contract_2020_07_df.join(enrollment_2020_07_df, on='Plan ID', how='outer')\n",
    "Aug_2020_merged_df = contract_2020_08_df.join(enrollment_2020_08_df, on='Plan ID', how='outer')\n",
    "Sep_2020_merged_df = contract_2020_09_df.join(enrollment_2020_09_df, on='Plan ID', how='outer')\n",
    "Oct_2020_merged_df = contract_2020_10_df.join(enrollment_2020_10_df, on='Plan ID', how='outer')\n",
    "Nov_2020_merged_df = contract_2020_11_df.join(enrollment_2020_11_df, on='Plan ID', how='outer')\n",
    "Dec_2020_merged_df = contract_2020_12_df.join(enrollment_2020_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2020_merged_df = Jan_2020_merged_df.drop(*columns_to_drop)\n",
    "Feb_2020_merged_df = Feb_2020_merged_df.drop(*columns_to_drop)\n",
    "Mar_2020_merged_df = Mar_2020_merged_df.drop(*columns_to_drop)\n",
    "Apr_2020_merged_df = Apr_2020_merged_df.drop(*columns_to_drop)\n",
    "May_2020_merged_df = May_2020_merged_df.drop(*columns_to_drop)\n",
    "Jun_2020_merged_df = Jun_2020_merged_df.drop(*columns_to_drop)\n",
    "Jul_2020_merged_df = Jul_2020_merged_df.drop(*columns_to_drop)\n",
    "Aug_2020_merged_df = Aug_2020_merged_df.drop(*columns_to_drop)\n",
    "Sep_2020_merged_df = Sep_2020_merged_df.drop(*columns_to_drop)\n",
    "Oct_2020_merged_df = Oct_2020_merged_df.drop(*columns_to_drop)\n",
    "Nov_2020_merged_df = Nov_2020_merged_df.drop(*columns_to_drop)\n",
    "Dec_2020_merged_df = Dec_2020_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2020_dfs = {\n",
    "    'Jan_2020': (Jan_2020_merged_df, 'January', '2020')\n",
    "    'Feb_2020': (Feb_2020_merged_df, 'February', '2020'),\n",
    "    'Mar_2020': (Mar_2020_merged_df, 'March', '2020'),\n",
    "    'Apr_2020': (Apr_2020_merged_df, 'April', '2020'),\n",
    "    'May_2020': (May_2020_merged_df, 'May', '2020'),\n",
    "    'Jun_2020': (Jun_2020_merged_df, 'June', '2020'),\n",
    "    'Jul_2020': (Jul_2020_merged_df, 'July', '2020'),\n",
    "    'Aug_2020': (Aug_2020_merged_df, 'August', '2020'),\n",
    "    'Sep_2020': (Sep_2020_merged_df, 'September', '2020'),\n",
    "    'Oct_2020': (Oct_2020_merged_df, 'October', '2020'),\n",
    "    'Nov_2020': (Nov_2020_merged_df, 'November', '2020'),\n",
    "    'Dec_2020': (Dec_2020_merged_df, 'December', '2020')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2020_dfs:\n",
    "    df, month, year = merged_2020_dfs[values]\n",
    "    merged_2020_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2020_merged_df = merged_2020_dfs['Jan_2020']\n",
    "Feb_2020_merged_df = merged_2020_dfs['Feb_2020']\n",
    "Mar_2020_merged_df = merged_2020_dfs['Mar_2020']\n",
    "Apr_2020_merged_df = merged_2020_dfs['Apr_2020']\n",
    "May_2020_merged_df = merged_2020_dfs['May_2020']\n",
    "Jun_2020_merged_df = merged_2020_dfs['Jun_2020']\n",
    "Jul_2020_merged_df = merged_2020_dfs['Jul_2020']\n",
    "Aug_2020_merged_df = merged_2020_dfs['Aug_2020']\n",
    "Sep_2020_merged_df = merged_2020_dfs['Sep_2020']\n",
    "Oct_2020_merged_df = merged_2020_dfs['Oct_2020']\n",
    "Nov_2020_merged_df = merged_2020_dfs['Nov_2020']\n",
    "Dec_2020_merged_df = merged_2020_dfs['Dec_2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2020_dfs = {\n",
    "    'Jan_2020': Jan_2020_merged_df\n",
    "    'Feb_2020': Feb_2020_merged_df,\n",
    "    'Mar_2020': Mar_2020_merged_df,\n",
    "    'Apr_2020': Apr_2020_merged_df,\n",
    "    'May_2020': May_2020_merged_df,\n",
    "    'Jun_2020': Jun_2020_merged_df,\n",
    "    'Jul_2020': Jul_2020_merged_df,\n",
    "    'Aug_2020': Aug_2020_merged_df,\n",
    "    'Sep_2020': Sep_2020_merged_df,\n",
    "    'Oct_2020': Oct_2020_merged_df,\n",
    "    'Nov_2020': Nov_2020_merged_df,\n",
    "    'Dec_2020': Dec_2020_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2020_dfs:\n",
    "    merged_2020_dfs[values] = add_total_enrollments(merged_2020_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2020_merged_df = merged_2020_dfs['Jan_2020']\n",
    "Feb_2020_merged_df = merged_2020_dfs['Feb_2020']\n",
    "Mar_2020_merged_df = merged_2020_dfs['Mar_2020']\n",
    "Apr_2020_merged_df = merged_2020_dfs['Apr_2020']\n",
    "May_2020_merged_df = merged_2020_dfs['May_2020']\n",
    "Jun_2020_merged_df = merged_2020_dfs['Jun_2020']\n",
    "Jul_2020_merged_df = merged_2020_dfs['Jul_2020']\n",
    "Aug_2020_merged_df = merged_2020_dfs['Aug_2020']\n",
    "Sep_2020_merged_df = merged_2020_dfs['Sep_2020']\n",
    "Oct_2020_merged_df = merged_2020_dfs['Oct_2020']\n",
    "Nov_2020_merged_df = merged_2020_dfs['Nov_2020']\n",
    "Dec_2020_merged_df = merged_2020_dfs['Dec_2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2020_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2020 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2020': Jan_2020_merged_df\n",
    "    'Feb_2020': Feb_2020_merged_df,\n",
    "    'Mar_2020': Mar_2020_merged_df,\n",
    "    'Apr_2020': Apr_2020_merged_df,\n",
    "    'May_2020': May_2020_merged_df,\n",
    "    'Jun_2020': Jun_2020_merged_df,\n",
    "    'Jul_2020': Jul_2020_merged_df,\n",
    "    'Aug_2020': Aug_2020_merged_df,\n",
    "    'Sep_2020': Sep_2020_merged_df,\n",
    "    'Oct_2020': Oct_2020_merged_df,\n",
    "    'Nov_2020': Nov_2020_merged_df,\n",
    "    'Dec_2020': Dec_2020_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2021 CSV files into DataFrames\n",
    "contract_2021_01_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_01.csv\", header=True, inferSchema=True)\n",
    "contract_2021_02_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_02.csv\", header=True, inferSchema=True)\n",
    "contract_2021_03_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_03.csv\", header=True, inferSchema=True)\n",
    "contract_2021_04_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_04.csv\", header=True, inferSchema=True)\n",
    "contract_2021_05_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_05.csv\", header=True, inferSchema=True)\n",
    "contract_2021_06_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_06.csv\", header=True, inferSchema=True)\n",
    "contract_2021_07_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_07.csv\", header=True, inferSchema=True)\n",
    "contract_2021_08_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_08.csv\", header=True, inferSchema=True)\n",
    "contract_2021_09_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_09.csv\", header=True, inferSchema=True)\n",
    "contract_2021_10_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_10.csv\", header=True, inferSchema=True)\n",
    "contract_2021_11_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_11.csv\", header=True, inferSchema=True)\n",
    "contract_2021_12_df = spark.read.csv(\"./2021/CPSC_Contract_Info_2021_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_01_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_02_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_03_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_04_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_05_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_06_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_07_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_08_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_09_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_10_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_11_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2021_12_df = spark.read.csv(\"./2021/CPSC_Enrollment_Info_2021_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2021_01_df.printSchema()\n",
    "enrollment_2021_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2021 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2021_merged_df = contract_2021_01_df.join(enrollment_2021_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2021_merged_df = contract_2021_02_df.join(enrollment_2021_02_df, on='Plan ID', how='outer')\n",
    "Mar_2021_merged_df = contract_2021_03_df.join(enrollment_2021_03_df, on='Plan ID', how='outer')\n",
    "Apr_2021_merged_df = contract_2021_04_df.join(enrollment_2021_04_df, on='Plan ID', how='outer')\n",
    "May_2021_merged_df = contract_2021_05_df.join(enrollment_2021_05_df, on='Plan ID', how='outer')\n",
    "Jun_2021_merged_df = contract_2021_06_df.join(enrollment_2021_06_df, on='Plan ID', how='outer')\n",
    "Jul_2021_merged_df = contract_2021_07_df.join(enrollment_2021_07_df, on='Plan ID', how='outer')\n",
    "Aug_2021_merged_df = contract_2021_08_df.join(enrollment_2021_08_df, on='Plan ID', how='outer')\n",
    "Sep_2021_merged_df = contract_2021_09_df.join(enrollment_2021_09_df, on='Plan ID', how='outer')\n",
    "Oct_2021_merged_df = contract_2021_10_df.join(enrollment_2021_10_df, on='Plan ID', how='outer')\n",
    "Nov_2021_merged_df = contract_2021_11_df.join(enrollment_2021_11_df, on='Plan ID', how='outer')\n",
    "Dec_2021_merged_df = contract_2021_12_df.join(enrollment_2021_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2021_merged_df = Jan_2021_merged_df.drop(*columns_to_drop)\n",
    "Feb_2021_merged_df = Feb_2021_merged_df.drop(*columns_to_drop)\n",
    "Mar_2021_merged_df = Mar_2021_merged_df.drop(*columns_to_drop)\n",
    "Apr_2021_merged_df = Apr_2021_merged_df.drop(*columns_to_drop)\n",
    "May_2021_merged_df = May_2021_merged_df.drop(*columns_to_drop)\n",
    "Jun_2021_merged_df = Jun_2021_merged_df.drop(*columns_to_drop)\n",
    "Jul_2021_merged_df = Jul_2021_merged_df.drop(*columns_to_drop)\n",
    "Aug_2021_merged_df = Aug_2021_merged_df.drop(*columns_to_drop)\n",
    "Sep_2021_merged_df = Sep_2021_merged_df.drop(*columns_to_drop)\n",
    "Oct_2021_merged_df = Oct_2021_merged_df.drop(*columns_to_drop)\n",
    "Nov_2021_merged_df = Nov_2021_merged_df.drop(*columns_to_drop)\n",
    "Dec_2021_merged_df = Dec_2021_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2021_dfs = {\n",
    "    'Jan_2021': (Jan_2021_merged_df, 'January', '2021')\n",
    "    'Feb_2021': (Feb_2021_merged_df, 'February', '2021'),\n",
    "    'Mar_2021': (Mar_2021_merged_df, 'March', '2021'),\n",
    "    'Apr_2021': (Apr_2021_merged_df, 'April', '2021'),\n",
    "    'May_2021': (May_2021_merged_df, 'May', '2021'),\n",
    "    'Jun_2021': (Jun_2021_merged_df, 'June', '2021'),\n",
    "    'Jul_2021': (Jul_2021_merged_df, 'July', '2021'),\n",
    "    'Aug_2021': (Aug_2021_merged_df, 'August', '2021'),\n",
    "    'Sep_2021': (Sep_2021_merged_df, 'September', '2021'),\n",
    "    'Oct_2021': (Oct_2021_merged_df, 'October', '2021'),\n",
    "    'Nov_2021': (Nov_2021_merged_df, 'November', '2021'),\n",
    "    'Dec_2021': (Dec_2021_merged_df, 'December', '2021')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2021_dfs:\n",
    "    df, month, year = merged_2021_dfs[values]\n",
    "    merged_2021_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2021_merged_df = merged_2021_dfs['Jan_2021']\n",
    "Feb_2021_merged_df = merged_2021_dfs['Feb_2021']\n",
    "Mar_2021_merged_df = merged_2021_dfs['Mar_2021']\n",
    "Apr_2021_merged_df = merged_2021_dfs['Apr_2021']\n",
    "May_2021_merged_df = merged_2021_dfs['May_2021']\n",
    "Jun_2021_merged_df = merged_2021_dfs['Jun_2021']\n",
    "Jul_2021_merged_df = merged_2021_dfs['Jul_2021']\n",
    "Aug_2021_merged_df = merged_2021_dfs['Aug_2021']\n",
    "Sep_2021_merged_df = merged_2021_dfs['Sep_2021']\n",
    "Oct_2021_merged_df = merged_2021_dfs['Oct_2021']\n",
    "Nov_2021_merged_df = merged_2021_dfs['Nov_2021']\n",
    "Dec_2021_merged_df = merged_2021_dfs['Dec_2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2021_dfs = {\n",
    "    'Jan_2021': Jan_2021_merged_df\n",
    "    'Feb_2021': Feb_2021_merged_df,\n",
    "    'Mar_2021': Mar_2021_merged_df,\n",
    "    'Apr_2021': Apr_2021_merged_df,\n",
    "    'May_2021': May_2021_merged_df,\n",
    "    'Jun_2021': Jun_2021_merged_df,\n",
    "    'Jul_2021': Jul_2021_merged_df,\n",
    "    'Aug_2021': Aug_2021_merged_df,\n",
    "    'Sep_2021': Sep_2021_merged_df,\n",
    "    'Oct_2021': Oct_2021_merged_df,\n",
    "    'Nov_2021': Nov_2021_merged_df,\n",
    "    'Dec_2021': Dec_2021_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2021_dfs:\n",
    "    merged_2021_dfs[values] = add_total_enrollments(merged_2021_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2021_merged_df = merged_2021_dfs['Jan_2021']\n",
    "Feb_2021_merged_df = merged_2021_dfs['Feb_2021']\n",
    "Mar_2021_merged_df = merged_2021_dfs['Mar_2021']\n",
    "Apr_2021_merged_df = merged_2021_dfs['Apr_2021']\n",
    "May_2021_merged_df = merged_2021_dfs['May_2021']\n",
    "Jun_2021_merged_df = merged_2021_dfs['Jun_2021']\n",
    "Jul_2021_merged_df = merged_2021_dfs['Jul_2021']\n",
    "Aug_2021_merged_df = merged_2021_dfs['Aug_2021']\n",
    "Sep_2021_merged_df = merged_2021_dfs['Sep_2021']\n",
    "Oct_2021_merged_df = merged_2021_dfs['Oct_2021']\n",
    "Nov_2021_merged_df = merged_2021_dfs['Nov_2021']\n",
    "Dec_2021_merged_df = merged_2021_dfs['Dec_2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2021_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2021 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2021': Jan_2021_merged_df\n",
    "    'Feb_2021': Feb_2021_merged_df,\n",
    "    'Mar_2021': Mar_2021_merged_df,\n",
    "    'Apr_2021': Apr_2021_merged_df,\n",
    "    'May_2021': May_2021_merged_df,\n",
    "    'Jun_2021': Jun_2021_merged_df,\n",
    "    'Jul_2021': Jul_2021_merged_df,\n",
    "    'Aug_2021': Aug_2021_merged_df,\n",
    "    'Sep_2021': Sep_2021_merged_df,\n",
    "    'Oct_2021': Oct_2021_merged_df,\n",
    "    'Nov_2021': Nov_2021_merged_df,\n",
    "    'Dec_2021': Dec_2021_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2022 CSV files into DataFrames\n",
    "contract_2022_01_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_01.csv\", header=True, inferSchema=True)\n",
    "contract_2022_02_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_02.csv\", header=True, inferSchema=True)\n",
    "contract_2022_03_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_03.csv\", header=True, inferSchema=True)\n",
    "contract_2022_04_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_04.csv\", header=True, inferSchema=True)\n",
    "contract_2022_05_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_05.csv\", header=True, inferSchema=True)\n",
    "contract_2022_06_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_06.csv\", header=True, inferSchema=True)\n",
    "contract_2022_07_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_07.csv\", header=True, inferSchema=True)\n",
    "contract_2022_08_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_08.csv\", header=True, inferSchema=True)\n",
    "contract_2022_09_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_09.csv\", header=True, inferSchema=True)\n",
    "contract_2022_10_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_10.csv\", header=True, inferSchema=True)\n",
    "contract_2022_11_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_11.csv\", header=True, inferSchema=True)\n",
    "contract_2022_12_df = spark.read.csv(\"./2022/CPSC_Contract_Info_2022_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_01_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_02_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_03_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_04_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_05_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_06_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_07_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_08_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_09_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_10_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_11_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2022_12_df = spark.read.csv(\"./2022/CPSC_Enrollment_Info_2022_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2022_01_df.printSchema()\n",
    "enrollment_2022_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2022 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2022_merged_df = contract_2022_01_df.join(enrollment_2022_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2022_merged_df = contract_2022_02_df.join(enrollment_2022_02_df, on='Plan ID', how='outer')\n",
    "Mar_2022_merged_df = contract_2022_03_df.join(enrollment_2022_03_df, on='Plan ID', how='outer')\n",
    "Apr_2022_merged_df = contract_2022_04_df.join(enrollment_2022_04_df, on='Plan ID', how='outer')\n",
    "May_2022_merged_df = contract_2022_05_df.join(enrollment_2022_05_df, on='Plan ID', how='outer')\n",
    "Jun_2022_merged_df = contract_2022_06_df.join(enrollment_2022_06_df, on='Plan ID', how='outer')\n",
    "Jul_2022_merged_df = contract_2022_07_df.join(enrollment_2022_07_df, on='Plan ID', how='outer')\n",
    "Aug_2022_merged_df = contract_2022_08_df.join(enrollment_2022_08_df, on='Plan ID', how='outer')\n",
    "Sep_2022_merged_df = contract_2022_09_df.join(enrollment_2022_09_df, on='Plan ID', how='outer')\n",
    "Oct_2022_merged_df = contract_2022_10_df.join(enrollment_2022_10_df, on='Plan ID', how='outer')\n",
    "Nov_2022_merged_df = contract_2022_11_df.join(enrollment_2022_11_df, on='Plan ID', how='outer')\n",
    "Dec_2022_merged_df = contract_2022_12_df.join(enrollment_2022_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2022_merged_df = Jan_2022_merged_df.drop(*columns_to_drop)\n",
    "Feb_2022_merged_df = Feb_2022_merged_df.drop(*columns_to_drop)\n",
    "Mar_2022_merged_df = Mar_2022_merged_df.drop(*columns_to_drop)\n",
    "Apr_2022_merged_df = Apr_2022_merged_df.drop(*columns_to_drop)\n",
    "May_2022_merged_df = May_2022_merged_df.drop(*columns_to_drop)\n",
    "Jun_2022_merged_df = Jun_2022_merged_df.drop(*columns_to_drop)\n",
    "Jul_2022_merged_df = Jul_2022_merged_df.drop(*columns_to_drop)\n",
    "Aug_2022_merged_df = Aug_2022_merged_df.drop(*columns_to_drop)\n",
    "Sep_2022_merged_df = Sep_2022_merged_df.drop(*columns_to_drop)\n",
    "Oct_2022_merged_df = Oct_2022_merged_df.drop(*columns_to_drop)\n",
    "Nov_2022_merged_df = Nov_2022_merged_df.drop(*columns_to_drop)\n",
    "Dec_2022_merged_df = Dec_2022_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2022_dfs = {\n",
    "    'Jan_2022': (Jan_2022_merged_df, 'January', '2022')\n",
    "    'Feb_2022': (Feb_2022_merged_df, 'February', '2022'),\n",
    "    'Mar_2022': (Mar_2022_merged_df, 'March', '2022'),\n",
    "    'Apr_2022': (Apr_2022_merged_df, 'April', '2022'),\n",
    "    'May_2022': (May_2022_merged_df, 'May', '2022'),\n",
    "    'Jun_2022': (Jun_2022_merged_df, 'June', '2022'),\n",
    "    'Jul_2022': (Jul_2022_merged_df, 'July', '2022'),\n",
    "    'Aug_2022': (Aug_2022_merged_df, 'August', '2022'),\n",
    "    'Sep_2022': (Sep_2022_merged_df, 'September', '2022'),\n",
    "    'Oct_2022': (Oct_2022_merged_df, 'October', '2022'),\n",
    "    'Nov_2022': (Nov_2022_merged_df, 'November', '2022'),\n",
    "    'Dec_2022': (Dec_2022_merged_df, 'December', '2022')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2022_dfs:\n",
    "    df, month, year = merged_2022_dfs[values]\n",
    "    merged_2022_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2022_merged_df = merged_2022_dfs['Jan_2022']\n",
    "Feb_2022_merged_df = merged_2022_dfs['Feb_2022']\n",
    "Mar_2022_merged_df = merged_2022_dfs['Mar_2022']\n",
    "Apr_2022_merged_df = merged_2022_dfs['Apr_2022']\n",
    "May_2022_merged_df = merged_2022_dfs['May_2022']\n",
    "Jun_2022_merged_df = merged_2022_dfs['Jun_2022']\n",
    "Jul_2022_merged_df = merged_2022_dfs['Jul_2022']\n",
    "Aug_2022_merged_df = merged_2022_dfs['Aug_2022']\n",
    "Sep_2022_merged_df = merged_2022_dfs['Sep_2022']\n",
    "Oct_2022_merged_df = merged_2022_dfs['Oct_2022']\n",
    "Nov_2022_merged_df = merged_2022_dfs['Nov_2022']\n",
    "Dec_2022_merged_df = merged_2022_dfs['Dec_2022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2022_dfs = {\n",
    "    'Jan_2022': Jan_2022_merged_df\n",
    "    'Feb_2022': Feb_2022_merged_df,\n",
    "    'Mar_2022': Mar_2022_merged_df,\n",
    "    'Apr_2022': Apr_2022_merged_df,\n",
    "    'May_2022': May_2022_merged_df,\n",
    "    'Jun_2022': Jun_2022_merged_df,\n",
    "    'Jul_2022': Jul_2022_merged_df,\n",
    "    'Aug_2022': Aug_2022_merged_df,\n",
    "    'Sep_2022': Sep_2022_merged_df,\n",
    "    'Oct_2022': Oct_2022_merged_df,\n",
    "    'Nov_2022': Nov_2022_merged_df,\n",
    "    'Dec_2022': Dec_2022_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2022_dfs:\n",
    "    merged_2022_dfs[values] = add_total_enrollments(merged_2022_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2022_merged_df = merged_2022_dfs['Jan_2022']\n",
    "Feb_2022_merged_df = merged_2022_dfs['Feb_2022']\n",
    "Mar_2022_merged_df = merged_2022_dfs['Mar_2022']\n",
    "Apr_2022_merged_df = merged_2022_dfs['Apr_2022']\n",
    "May_2022_merged_df = merged_2022_dfs['May_2022']\n",
    "Jun_2022_merged_df = merged_2022_dfs['Jun_2022']\n",
    "Jul_2022_merged_df = merged_2022_dfs['Jul_2022']\n",
    "Aug_2022_merged_df = merged_2022_dfs['Aug_2022']\n",
    "Sep_2022_merged_df = merged_2022_dfs['Sep_2022']\n",
    "Oct_2022_merged_df = merged_2022_dfs['Oct_2022']\n",
    "Nov_2022_merged_df = merged_2022_dfs['Nov_2022']\n",
    "Dec_2022_merged_df = merged_2022_dfs['Dec_2022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2022_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2022 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2022': Jan_2022_merged_df\n",
    "    'Feb_2022': Feb_2022_merged_df,\n",
    "    'Mar_2022': Mar_2022_merged_df,\n",
    "    'Apr_2022': Apr_2022_merged_df,\n",
    "    'May_2022': May_2022_merged_df,\n",
    "    'Jun_2022': Jun_2022_merged_df,\n",
    "    'Jul_2022': Jul_2022_merged_df,\n",
    "    'Aug_2022': Aug_2022_merged_df,\n",
    "    'Sep_2022': Sep_2022_merged_df,\n",
    "    'Oct_2022': Oct_2022_merged_df,\n",
    "    'Nov_2022': Nov_2022_merged_df,\n",
    "    'Dec_2022': Dec_2022_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "    #.appName(\"Enrollment_2014_CSV_Merge\") \\\n",
    "    #.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2023 CSV files into DataFrames\n",
    "contract_2023_01_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_01.csv\", header=True, inferSchema=True)\n",
    "contract_2023_02_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_02.csv\", header=True, inferSchema=True)\n",
    "contract_2023_03_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_03.csv\", header=True, inferSchema=True)\n",
    "contract_2023_04_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_04.csv\", header=True, inferSchema=True)\n",
    "contract_2023_05_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_05.csv\", header=True, inferSchema=True)\n",
    "contract_2023_06_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_06.csv\", header=True, inferSchema=True)\n",
    "contract_2023_07_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_07.csv\", header=True, inferSchema=True)\n",
    "contract_2023_08_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_08.csv\", header=True, inferSchema=True)\n",
    "contract_2023_09_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_09.csv\", header=True, inferSchema=True)\n",
    "contract_2023_10_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_10.csv\", header=True, inferSchema=True)\n",
    "contract_2023_11_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_11.csv\", header=True, inferSchema=True)\n",
    "contract_2023_12_df = spark.read.csv(\"./2023/CPSC_Contract_Info_2023_12.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_01_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_01.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_02_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_02.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_03_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_03.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_04_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_04.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_05_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_05.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_06_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_06.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_07_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_07.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_08_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_08.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_09_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_09.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_10_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_10.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_11_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_11.csv\", header=True, inferSchema=True)\n",
    "enrollment_2023_12_df = spark.read.csv(\"./2023/CPSC_Enrollment_Info_2023_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract ID: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- Organization Type: string (nullable = true)\n",
      " |-- Plan Type: string (nullable = true)\n",
      " |-- Offers Part D: string (nullable = true)\n",
      " |-- SNP Plan: string (nullable = true)\n",
      " |-- EGHP: string (nullable = true)\n",
      " |-- Organization Name: string (nullable = true)\n",
      " |-- Organization Marketing Name: string (nullable = true)\n",
      " |-- Plan Name: string (nullable = true)\n",
      " |-- Parent Organization: string (nullable = true)\n",
      " |-- Contract Effective Date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Contract Number: string (nullable = true)\n",
      " |-- Plan ID: integer (nullable = true)\n",
      " |-- SSA State County Code: integer (nullable = true)\n",
      " |-- FIPS State County Code: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Enrollment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at columns in each csv\n",
    "contract_2023_01_df.printSchema()\n",
    "enrollment_2023_01_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join on 2023 monthly CSVs\n",
    "# Perform the outer join on \"Plan ID\" and show result to ensure data populates appropriately\n",
    "Jan_2023_merged_df = contract_2023_01_df.join(enrollment_2023_01_df, on=\"Plan ID\", how=\"outer\")\n",
    "Feb_2023_merged_df = contract_2023_02_df.join(enrollment_2023_02_df, on='Plan ID', how='outer')\n",
    "Mar_2023_merged_df = contract_2023_03_df.join(enrollment_2023_03_df, on='Plan ID', how='outer')\n",
    "Apr_2023_merged_df = contract_2023_04_df.join(enrollment_2023_04_df, on='Plan ID', how='outer')\n",
    "May_2023_merged_df = contract_2023_05_df.join(enrollment_2023_05_df, on='Plan ID', how='outer')\n",
    "Jun_2023_merged_df = contract_2023_06_df.join(enrollment_2023_06_df, on='Plan ID', how='outer')\n",
    "Jul_2023_merged_df = contract_2023_07_df.join(enrollment_2023_07_df, on='Plan ID', how='outer')\n",
    "Aug_2023_merged_df = contract_2023_08_df.join(enrollment_2023_08_df, on='Plan ID', how='outer')\n",
    "Sep_2023_merged_df = contract_2023_09_df.join(enrollment_2023_09_df, on='Plan ID', how='outer')\n",
    "Oct_2023_merged_df = contract_2023_10_df.join(enrollment_2023_10_df, on='Plan ID', how='outer')\n",
    "Nov_2023_merged_df = contract_2023_11_df.join(enrollment_2023_11_df, on='Plan ID', how='outer')\n",
    "Dec_2023_merged_df = contract_2023_12_df.join(enrollment_2023_12_df, on='Plan ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023\n",
    "# Drop SNP Plan, EGHP, Organization Type, Plan Type, Organization Name from Contract Files\n",
    "\n",
    "columns_to_drop = ['Organization Type', 'SNP Plan', 'EGHP', 'Plan Name', 'Contract Effective Date', 'SSA State County Code']\n",
    "\n",
    "Jan_2023_merged_df = Jan_2023_merged_df.drop(*columns_to_drop)\n",
    "Feb_2023_merged_df = Feb_2023_merged_df.drop(*columns_to_drop)\n",
    "Mar_2023_merged_df = Mar_2023_merged_df.drop(*columns_to_drop)\n",
    "Apr_2023_merged_df = Apr_2023_merged_df.drop(*columns_to_drop)\n",
    "May_2023_merged_df = May_2023_merged_df.drop(*columns_to_drop)\n",
    "Jun_2023_merged_df = Jun_2023_merged_df.drop(*columns_to_drop)\n",
    "Jul_2023_merged_df = Jul_2023_merged_df.drop(*columns_to_drop)\n",
    "Aug_2023_merged_df = Aug_2023_merged_df.drop(*columns_to_drop)\n",
    "Sep_2023_merged_df = Sep_2023_merged_df.drop(*columns_to_drop)\n",
    "Oct_2023_merged_df = Oct_2023_merged_df.drop(*columns_to_drop)\n",
    "Nov_2023_merged_df = Nov_2023_merged_df.drop(*columns_to_drop)\n",
    "Dec_2023_merged_df = Dec_2023_merged_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# Add month and year column after merging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and year column to store month and year in file name so we know which file/data this includes\n",
    "# Cannot use Panadas df because data is too large; import lit from pyspark.sql.functions and .withColumn() to add columns in each df\n",
    "# Create loop to run through each df and add columns\n",
    "\n",
    "# Create dictionary to map month/year to file format\n",
    "merged_2023_dfs = {\n",
    "    'Jan_2023': (Jan_2023_merged_df, 'January', '2023')\n",
    "    'Feb_2023': (Feb_2023_merged_df, 'February', '2023'),\n",
    "    'Mar_2023': (Mar_2023_merged_df, 'March', '2023'),\n",
    "    'Apr_2023': (Apr_2023_merged_df, 'April', '2023'),\n",
    "    'May_2023': (May_2023_merged_df, 'May', '2023'),\n",
    "    'Jun_2023': (Jun_2023_merged_df, 'June', '2023'),\n",
    "    'Jul_2023': (Jul_2023_merged_df, 'July', '2023'),\n",
    "    'Aug_2023': (Aug_2023_merged_df, 'August', '2023'),\n",
    "    'Sep_2023': (Sep_2023_merged_df, 'September', '2023'),\n",
    "    'Oct_2023': (Oct_2023_merged_df, 'October', '2023'),\n",
    "    'Nov_2023': (Nov_2023_merged_df, 'November', '2023'),\n",
    "    'Dec_2023': (Dec_2023_merged_df, 'December', '2023')\n",
    "}\n",
    "\n",
    "# Loop through each DataFrame and add the columns\n",
    "for values in merged_2023_dfs:\n",
    "    df, month, year = merged_2023_dfs[values]\n",
    "    merged_2023_dfs[values] = df.withColumn('Month', lit(month)).withColumn('Year', lit(year))\n",
    "\n",
    "# Updated DFs\n",
    "Jan_2023_merged_df = merged_2023_dfs['Jan_2023']\n",
    "Feb_2023_merged_df = merged_2023_dfs['Feb_2023']\n",
    "Mar_2023_merged_df = merged_2023_dfs['Mar_2023']\n",
    "Apr_2023_merged_df = merged_2023_dfs['Apr_2023']\n",
    "May_2023_merged_df = merged_2023_dfs['May_2023']\n",
    "Jun_2023_merged_df = merged_2023_dfs['Jun_2023']\n",
    "Jul_2023_merged_df = merged_2023_dfs['Jul_2023']\n",
    "Aug_2023_merged_df = merged_2023_dfs['Aug_2023']\n",
    "Sep_2023_merged_df = merged_2023_dfs['Sep_2023']\n",
    "Oct_2023_merged_df = merged_2023_dfs['Oct_2023']\n",
    "Nov_2023_merged_df = merged_2023_dfs['Nov_2023']\n",
    "Dec_2023_merged_df = merged_2023_dfs['Dec_2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column totaling enrollments by state, then Drop County after adding enrollment by state \n",
    "# Group by state then sum enrollment values for each state; create function to reuse.\n",
    "# Create new column to put value in\n",
    "\n",
    "# Function to add total enrollments by state\n",
    "def add_total_enrollments(df):\n",
    "    total_enrollments_df = df.groupBy(\"State\").agg(spark_sum(\"Enrollment\").alias(\"Total Enrollments by State\"))\n",
    "    return df.join(total_enrollments_df, on=\"State\", how=\"left\")\n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_2023_dfs = {\n",
    "    'Jan_2023': Jan_2023_merged_df\n",
    "    'Feb_2023': Feb_2023_merged_df,\n",
    "    'Mar_2023': Mar_2023_merged_df,\n",
    "    'Apr_2023': Apr_2023_merged_df,\n",
    "    'May_2023': May_2023_merged_df,\n",
    "    'Jun_2023': Jun_2023_merged_df,\n",
    "    'Jul_2023': Jul_2023_merged_df,\n",
    "    'Aug_2023': Aug_2023_merged_df,\n",
    "    'Sep_2023': Sep_2023_merged_df,\n",
    "    'Oct_2023': Oct_2023_merged_df,\n",
    "    'Nov_2023': Nov_2023_merged_df,\n",
    "    'Dec_2023': Dec_2023_merged_df\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for values in merged_2023_dfs:\n",
    "    merged_2023_dfs[values] = add_total_enrollments(merged_2023_dfs[values])\n",
    "\n",
    "# Extract back the modified DataFrames\n",
    "Jan_2023_merged_df = merged_2023_dfs['Jan_2023']\n",
    "Feb_2023_merged_df = merged_2023_dfs['Feb_2023']\n",
    "Mar_2023_merged_df = merged_2023_dfs['Mar_2023']\n",
    "Apr_2023_merged_df = merged_2023_dfs['Apr_2023']\n",
    "May_2023_merged_df = merged_2023_dfs['May_2023']\n",
    "Jun_2023_merged_df = merged_2023_dfs['Jun_2023']\n",
    "Jul_2023_merged_df = merged_2023_dfs['Jul_2023']\n",
    "Aug_2023_merged_df = merged_2023_dfs['Aug_2023']\n",
    "Sep_2023_merged_df = merged_2023_dfs['Sep_2023']\n",
    "Oct_2023_merged_df = merged_2023_dfs['Oct_2023']\n",
    "Nov_2023_merged_df = merged_2023_dfs['Nov_2023']\n",
    "Dec_2023_merged_df = merged_2023_dfs['Dec_2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to show state and total enrollments by state only (one row per state)\n",
    "Mar_2023_merged_df.select(\"State\", \"Total Enrollments by State\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSVs \n",
    "path_2023 = \"./PySpark_Merged_CSVs\"  \n",
    "\n",
    "# Dictionary of DataFrames\n",
    "merged_dfs = {\n",
    "    'Jan_2023': Jan_2023_merged_df\n",
    "    'Feb_2023': Feb_2023_merged_df,\n",
    "    'Mar_2023': Mar_2023_merged_df,\n",
    "    'Apr_2023': Apr_2023_merged_df,\n",
    "    'May_2023': May_2023_merged_df,\n",
    "    'Jun_2023': Jun_2023_merged_df,\n",
    "    'Jul_2023': Jul_2023_merged_df,\n",
    "    'Aug_2023': Aug_2023_merged_df,\n",
    "    'Sep_2023': Sep_2023_merged_df,\n",
    "    'Oct_2023': Oct_2023_merged_df,\n",
    "    'Nov_2023': Nov_2023_merged_df,\n",
    "    'Dec_2023': Dec_2023_merged_df\n",
    "}\n",
    "\n",
    "# Save each DataFrame as a CSV\n",
    "for month, df in merged_dfs.items():\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_path}{month}_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK IN PROGRESS:\n",
    "# Merge Monthly DataFrames by Year into a single year of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Store merged DataFrames in a list.\n",
    "#### 2. Merge the DataFrames on \"Plan ID\" and allow missing columns.\n",
    "#### 3. View Saved DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+---------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+---------------+----------+\n",
      "|Plan ID|Contract ID|Organization Type|Plan Type|Offers Part D|SNP Plan|EGHP|   Organization Name|Organization Marketing Name|           Plan Name| Parent Organization|Contract Effective Date|Contract Number|SSA State County Code|FIPS State County Code|State|         County|Enrollment|\n",
      "+-------+-----------+-----------------+---------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+---------------+----------+\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16000|                 19001|   IA|          Adair|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16010|                 19003|   IA|          Adams|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16030|                 19007|   IA|      Appanoose|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16050|                 19011|   IA|         Benton|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16070|                 19015|   IA|          Boone|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16140|                 19029|   IA|           Cass|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16150|                 19031|   IA|          Cedar|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16190|                 19039|   IA|         Clarke|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16220|                 19045|   IA|        Clinton|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16230|                 19047|   IA|       Crawford|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16250|                 19051|   IA|          Davis|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16280|                 19057|   IA|     Des Moines|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16350|                 19071|   IA|        Fremont|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16360|                 19073|   IA|         Greene|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16370|                 19075|   IA|         Grundy|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16380|                 19077|   IA|        Guthrie|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16410|                 19083|   IA|         Hardin|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16420|                 19085|   IA|       Harrison|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16430|                 19087|   IA|          Henry|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16470|                 19095|   IA|           Iowa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16490|                 19099|   IA|         Jasper|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16500|                 19101|   IA|      Jefferson|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16510|                 19103|   IA|        Johnson|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16530|                 19107|   IA|         Keokuk|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16550|                 19111|   IA|            Lee|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16570|                 19115|   IA|         Louisa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16580|                 19117|   IA|          Lucas|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16600|                 19121|   IA|        Madison|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16610|                 19123|   IA|        Mahaska|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16630|                 19127|   IA|       Marshall|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16640|                 19129|   IA|          Mills|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16660|                 19133|   IA|         Monona|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16680|                 19137|   IA|     Montgomery|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16690|                 19139|   IA|      Muscatine|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16760|                 19153|   IA|           Polk|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16770|                 19155|   IA|  Pottawattamie|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16780|                 19157|   IA|      Poweshiek|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16790|                 19159|   IA|       Ringgold|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16810|                 19163|   IA|          Scott|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16820|                 19165|   IA|         Shelby|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16850|                 19171|   IA|           Tama|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16880|                 19177|   IA|      Van Buren|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16900|                 19181|   IA|         Warren|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0084|                16910|                 19183|   IA|     Washington|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0108|                 3090|                  4019|   AZ|           Pima|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0108|                 5200|                  6037|   CA|    Los Angeles|        35|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0108|                 5210|                  6037|   CA|    Los Angeles|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 3060|                  4013|   AZ|       Maricopa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 3100|                  4021|   AZ|          Pinal|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 3130|                  4027|   AZ|           Yuma|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5000|                  6001|   CA|        Alameda|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5060|                  6013|   CA|   Contra Costa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5090|                  6019|   CA|         Fresno|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5140|                  6029|   CA|           Kern|      1759|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5200|                  6037|   CA|    Los Angeles|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5300|                  6039|   CA|         Madera|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5430|                  6065|   CA|      Riverside|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5460|                  6071|   CA| San Bernardino|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5470|                  6073|   CA|      San Diego|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5520|                  6083|   CA|  Santa Barbara|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5640|                  6107|   CA|         Tulare|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                 5660|                  6111|   CA|        Ventura|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                10050|                 12011|   FL|        Broward|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                14141|                 17031|   IL|           Cook|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                37300|                 40061|   OK|        Haskell|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                45300|                 48081|   TX|           Coke|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                45794|                 48329|   TX|        Midland|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0523|                52390|                 55079|   WI|      Milwaukee|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                61000|                  NULL| NULL|           NULL|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                57280|                  NULL| NULL|           NULL|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                58130|                  NULL| NULL|           NULL|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                56010|                  NULL| NULL|           NULL|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 3060|                  4013|   AZ|       Maricopa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 3090|                  4019|   AZ|           Pima|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5000|                  6001|   CA|        Alameda|        50|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5010|                  6003|   CA|         Alpine|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5020|                  6005|   CA|         Amador|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5030|                  6007|   CA|          Butte|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5040|                  6009|   CA|      Calaveras|        13|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5060|                  6013|   CA|   Contra Costa|        34|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5080|                  6017|   CA|      El Dorado|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5090|                  6019|   CA|         Fresno|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5110|                  6023|   CA|       Humboldt|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5140|                  6029|   CA|           Kern|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5150|                  6031|   CA|          Kings|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5160|                  6033|   CA|           Lake|        21|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5170|                  6035|   CA|         Lassen|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5200|                  6037|   CA|    Los Angeles|        13|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5300|                  6039|   CA|         Madera|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5310|                  6041|   CA|          Marin|     10415|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5320|                  6043|   CA|       Mariposa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5330|                  6045|   CA|      Mendocino|        15|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5340|                  6047|   CA|         Merced|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5350|                  6049|   CA|          Modoc|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5370|                  6053|   CA|       Monterey|        11|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5380|                  6055|   CA|           Napa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5390|                  6057|   CA|         Nevada|        16|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5400|                  6059|   CA|         Orange|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5410|                  6061|   CA|         Placer|        11|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5430|                  6065|   CA|      Riverside|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5440|                  6067|   CA|     Sacramento|        22|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5450|                  6069|   CA|     San Benito|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5460|                  6071|   CA| San Bernardino|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5470|                  6073|   CA|      San Diego|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5480|                  6075|   CA|  San Francisco|       127|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5490|                  6077|   CA|    San Joaquin|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5500|                  6079|   CA|San Luis Obispo|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5510|                  6081|   CA|      San Mateo|     15020|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5520|                  6083|   CA|  Santa Barbara|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5530|                  6085|   CA|    Santa Clara|        82|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5540|                  6087|   CA|     Santa Cruz|        12|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5550|                  6089|   CA|         Shasta|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5560|                  6091|   CA|         Sierra|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5570|                  6093|   CA|       Siskiyou|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5580|                  6095|   CA|         Solano|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5590|                  6097|   CA|         Sonoma|        55|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5600|                  6099|   CA|     Stanislaus|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5610|                  6101|   CA|         Sutter|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5620|                  6103|   CA|         Tehama|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5650|                  6109|   CA|       Tuolumne|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5660|                  6111|   CA|        Ventura|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 5680|                  6115|   CA|           Yuba|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 6200|                  8041|   CO|        El Paso|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 6290|                  8059|   CO|      Jefferson|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 7000|                  9001|   CT|      Fairfield|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 7010|                  9003|   CT|       Hartford|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                 7040|                  9009|   CT|      New Haven|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                10040|                 12009|   FL|        Brevard|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                10100|                 12021|   FL|        Collier|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                10260|                 12053|   FL|       Hernando|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                10400|                 12081|   FL|        Manatee|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                10120|                 12086|   FL|     Miami-Dade|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                10490|                 12099|   FL|     Palm Beach|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                10510|                 12103|   FL|       Pinellas|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                11260|                 13059|   GA|         Clarke|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                11370|                 13089|   GA|         DeKalb|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                11901|                 13279|   GA|         Toombs|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                12010|                 15001|   HI|         Hawaii|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                12020|                 15003|   HI|       Honolulu|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                12040|                 15007|   HI|          Kauai|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                12050|                 15009|   HI|           Maui|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                13030|                 16007|   ID|      Bear Lake|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                13280|                 16057|   ID|          Latah|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                13420|                 16085|   ID|         Valley|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                13430|                 16087|   ID|     Washington|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                15140|                 18029|   IN|       Dearborn|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                15820|                 18165|   IN|     Vermillion|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                19350|                 22071|   LA|        Orleans|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                21080|                 24017|   MD|        Charles|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                23320|                 26065|   MI|         Ingham|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                23440|                 26089|   MI|       Leelanau|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                24260|                 27053|   MN|       Hennepin|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                24610|                 27123|   MN|         Ramsey|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                26020|                 29005|   MO|       Atchison|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                26940|                 29189|   MO|      St. Louis|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                29010|                 32003|   NV|          Clark|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                29030|                 32007|   NV|           Elko|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                29110|                 32023|   NV|            Nye|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                29150|                 32031|   NV|         Washoe|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                29120|                 32510|   NV|    Carson City|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                32000|                 35001|   NM|     Bernalillo|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                33331|                 36047|   NY|          Kings|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                33420|                 36061|   NY|       New York|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                34910|                 37183|   NC|           Wake|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                36170|                 39035|   OH|       Cuyahoga|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                36510|                 39099|   OH|       Mahoning|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                36780|                 39153|   OH|         Summit|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                37550|                 40111|   OK|       Okmulgee|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                37710|                 40143|   OK|          Tulsa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                37720|                 40145|   OK|        Wagoner|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                38070|                 41015|   OR|          Curry|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                38080|                 41017|   OR|      Deschutes|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                38170|                 41035|   OR|        Klamath|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                38190|                 41039|   OR|           Lane|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                38200|                 41041|   OR|        Lincoln|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                38230|                 41047|   OR|         Marion|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                38250|                 41051|   OR|      Multnomah|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                39010|                 42003|   PA|      Allegheny|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                39140|                 42017|   PA|          Bucks|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                45480|                 48141|   TX|        El Paso|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                45770|                 48303|   TX|        Lubbock|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                45940|                 48453|   TX|         Travis|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                46170|                 49035|   UT|      Salt Lake|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                46210|                 49043|   UT|         Summit|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                46240|                 49049|   UT|           Utah|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                49290|                 51059|   VA|        Fairfax|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                50040|                 53009|   WA|        Clallam|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                50080|                 53017|   WA|        Douglas|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                50160|                 53033|   WA|           King|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                50170|                 53035|   WA|         Kitsap|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                50230|                 53047|   WA|       Okanogan|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                50260|                 53053|   WA|         Pierce|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                50310|                 53063|   WA|        Spokane|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                52000|                 55001|   WI|          Adams|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0524|                64050|                 60050|   AS|        Leasina|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0544|                 5060|                  6013|   CA|   Contra Costa|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0544|                 5600|                  6099|   CA|     Stanislaus|       196|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0755|                58270|                  NULL| NULL|           NULL|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0755|                55310|                  NULL| NULL|           NULL|         *|\n",
      "|     31|      H0084|        Local CCP|Local PPO|          Yes|     Yes|  No|CARE IMPROVEMENT ...|       Care Improvement ...|Care Improvement ...|UnitedHealth Grou...|     01/01/2009 0:00:00|          H0755|                 5520|                  6083|   CA|  Santa Barbara|         *|\n",
      "+-------+-----------+-----------------+---------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+---------------+----------+\n",
      "only showing top 200 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge DataFrames using outer join so no data is lost\n",
    "\n",
    "# Put 2013 df in a list\n",
    "merged_dfs = [] #insert df names \n",
    "   \n",
    "\n",
    "# Column to merge on (an integer column)\n",
    "join_column = \"Plan ID\"\n",
    "\n",
    "# Create a loop, starting with the first df\n",
    "merged_2013_df = merged_dfs[0]\n",
    "\n",
    "# Interate through the df to merge the rest\n",
    "for df in merged_dfs[1:]:\n",
    "    merged_dfs = merged_2013_df.join(df, on=join_column, how=\"outer\")\n",
    "\n",
    "# Print results\n",
    "merged_2013_df.show(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o2072.csv.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 682.0 failed 1 times, most recent failure: Lost task 9.0 in stage 682.0 (TID 3103) (kmac.localdomain executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2072.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 682.0 failed 1 times, most recent failure: Lost task 9.0 in stage 682.0 (TID 3103) (kmac.localdomain executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmerged_2013_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Combined_CSVs/merged_2013_df\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[0;32m    170\u001b[0m     )\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[1;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     61\u001b[0m     stackTrace\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(origin\u001b[38;5;241m.\u001b[39mgetCause())\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:157\u001b[0m, in \u001b[0;36mconvert_exception\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkUpgradeException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m    156\u001b[0m c: Py4JJavaError \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mgetCause()\n\u001b[1;32m--> 157\u001b[0m stacktrace: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(e)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    159\u001b[0m     is_instance_of(gw, c, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.api.python.PythonException\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    166\u001b[0m ):\n\u001b[0;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[0;32m    170\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kathr\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[0;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[0;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[1;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[1;31mPy4JError\u001b[0m: org does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "merged_2013_df.write.csv('./Combined_CSVs/merged_2013_df', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+--------------------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+----------+----------+\n",
      "|Plan ID|Contract ID|Organization Type|           Plan Type|Offers Part D|SNP Plan|EGHP|   Organization Name|Organization Marketing Name|           Plan Name| Parent Organization|Contract Effective Date|Contract Number|SSA State County Code|FIPS State County Code|State|    County|Enrollment|\n",
      "+-------+-----------+-----------------+--------------------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+----------+----------+\n",
      "|   NULL|      90091| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|UNITED MINE WORKE...|       United Mine Worke...|                NULL|UMWA Health and R...|     02/01/1974 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H1651|        1876 Cost|           1876 Cost|           No|      No|  No|MEDICAL ASSOCIATE...|       Medical Associate...|                NULL|Medical Associate...|     02/01/1996 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H2232|            Pilot|               Pilot|           No|      No|  No|MASSACHUSETTS GEN...|             Health Diaglog|                NULL|      Health Diaglog|     10/01/2005 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H4556| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|CONSOLIDATED ASSO...|       Consolidated Asso...|                NULL|Consolidated Asso...|     01/01/1992 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H4652| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|UNION PACIFIC RAI...|       Union Pacific Rai...|                NULL|Union Pacific Rai...|     12/01/1993 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H4906| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|C AND O EMPLOYEES...|       C and O Employees...|                NULL|C & O Employees' ...|     05/01/1999 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H5264|        1876 Cost|           1876 Cost|           No|      No|  No|DEAN HEALTH PLAN,...|       Dean Health Plan,...|                NULL|Dean Health Syste...|     01/01/1999 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H5444|            Pilot|               Pilot|           No|      No|  No|RMS DM, LLC DEMO ...|                Rms Dm, Llc|                NULL|          Davita Inc|     10/01/2005 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H6053| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|SANTE FE EMPLOYEE...|       Santa Fe Employes...|                NULL|Sante Fe Employee...|     01/01/1987 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H6140| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|WABASH MEM. HOSPITAL|       Wabash Mem. Hospital|                NULL|Wabash Memorial H...|     01/01/1987 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H6141| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|   SIDNEY HILLMAN HC|          Sidney Hillman HC|                NULL|Sidney Hillman He...|     02/01/1983 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H6142| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|UNION HEALTH SERV...|       Union Health Serv...|                NULL|Union Health Serv...|     02/01/1983 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H6143| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|UNION MEDICAL CENTER|       Union Medical Center|                NULL|Union Medical Center|     02/01/1983 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|   NULL|      H6334| HCPP - 1833 Cost|    HCPP - 1833 Cost|           No|      No|  No|NY HOTEL TRADES C...|       NY Hotel Trades C...|                NULL|NY Hotel Trades C...|     01/01/1987 0:00:00|           NULL|                 NULL|                  NULL| NULL|      NULL|      NULL|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36110|                 39023|   OH|     Clark|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36170|                 39035|   OH|  Cuyahoga|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36260|                 39051|   OH|    Fulton|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36280|                 39055|   OH|    Geauga|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36290|                 39057|   OH|    Greene|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36440|                 39085|   OH|      Lake|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36480|                 39093|   OH|    Lorain|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36490|                 39095|   OH|     Lucas|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36530|                 39103|   OH|    Medina|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36580|                 39113|   OH|Montgomery|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36630|                 39123|   OH|    Ottawa|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0022|                36880|                 39173|   OH|      Wood|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                 3060|                  4013|   AZ|  Maricopa|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                10350|                 12071|   FL|       Lee|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                10570|                 12115|   FL|  Sarasota|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                14141|                 17031|   IL|      Cook|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                16050|                 19011|   IA|    Benton|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                16140|                 19029|   IA|      Cass|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                16220|                 19045|   IA|   Clinton|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                16270|                 19055|   IA|  Delaware|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                16510|                 19103|   IA|   Johnson|       270|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                16560|                 19113|   IA|      Linn|      1130|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                24060|                 27013|   MN|Blue Earth|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                35270|                 38055|   ND|    McLean|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0028|                45650|                 48215|   TX|   Hidalgo|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                 3060|                  4013|   AZ|  Maricopa|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                 3080|                  4017|   AZ|    Navajo|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                 4450|                  5091|   AR|    Miller|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                 5610|                  6101|   CA|    Sutter|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                21020|                 24005|   MD| Baltimore|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                24260|                 27053|   MN|  Hennepin|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                26470|                 29095|   MO|   Jackson|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                33570|                 36077|   NY|    Otsego|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                35490|                 38099|   ND|     Walsh|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45010|                 48003|   TX|   Andrews|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45060|                 48013|   TX|  Atascosa|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45070|                 48015|   TX|    Austin|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45080|                 48017|   TX|    Bailey|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45090|                 48019|   TX|   Bandera|        13|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45130|                 48029|   TX|     Bexar|       157|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45140|                 48031|   TX|    Blanco|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45150|                 48033|   TX|    Borden|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45160|                 48035|   TX|    Bosque|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45170|                 48037|   TX|     Bowie|        83|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45180|                 48039|   TX|  Brazoria|       112|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45190|                 48041|   TX|    Brazos|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45201|                 48045|   TX|   Briscoe|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45210|                 48047|   TX|    Brooks|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45221|                 48051|   TX|  Burleson|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45222|                 48053|   TX|    Burnet|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45223|                 48055|   TX|  Caldwell|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45250|                 48063|   TX|      Camp|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45260|                 48067|   TX|      Cass|        76|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45270|                 48069|   TX|    Castro|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45280|                 48071|   TX|  Chambers|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45292|                 48079|   TX|   Cochran|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45300|                 48081|   TX|      Coke|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45301|                 48083|   TX|   Coleman|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45310|                 48085|   TX|    Collin|        42|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45320|                 48091|   TX|     Comal|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45321|                 48093|   TX|  Comanche|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45330|                 48095|   TX|    Concho|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45340|                 48097|   TX|     Cooke|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45362|                 48107|   TX|    Crosby|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45370|                 48109|   TX| Culberson|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45390|                 48113|   TX|    Dallas|       111|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45400|                 48119|   TX|     Delta|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45410|                 48121|   TX|    Denton|        28|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45420|                 48123|   TX|    DeWitt|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45421|                 48125|   TX|   Dickens|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45430|                 48127|   TX|    Dimmit|        80|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45440|                 48131|   TX|     Duval|        15|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45470|                 48139|   TX|     Ellis|        37|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45480|                 48141|   TX|   El Paso|        28|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45500|                 48145|   TX|     Falls|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45510|                 48147|   TX|    Fannin|        14|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45520|                 48151|   TX|    Fisher|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45521|                 48153|   TX|     Floyd|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45530|                 48157|   TX| Fort Bend|        38|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45531|                 48159|   TX|  Franklin|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45540|                 48161|   TX| Freestone|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45551|                 48169|   TX|     Garza|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45560|                 48173|   TX| Glasscock|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45561|                 48175|   TX|    Goliad|         *|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45562|                 48177|   TX|  Gonzales|        13|\n",
      "|      1|      H0022|             Demo|Medicare-Medicaid...|          Yes|      No|  No|BUCKEYE COMMUNITY...|       Advantage by Buck...|Buckeye Community...| Centene Corporation|                   NULL|          H0084|                45570|                 48183|   TX|     Gregg|         *|\n",
      "+-------+-----------+-----------------+--------------------+-------------+--------+----+--------------------+---------------------------+--------------------+--------------------+-----------------------+---------------+---------------------+----------------------+-----+----------+----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2013 Data\n",
    "\n",
    "# List DataFrames that were merged for 2013 above.\n",
    "merged_dfs = [\n",
    "    merged_2013_01_df, merged_2013_02_df, merged_2013_03_df, merged_2013_04_df, merged_2013_05_df, merged_2013_06_df, \n",
    "    merged_2013_07_df, merged_2013_08_df, merged_2013_09_df, merged_2013_10_df, merged_2013_11_df, merged_2013_12_df \n",
    "]\n",
    "\n",
    "# Merge DFs (use 'reduce' to join the DFs on \"plan ID\" [unionByName()]); trying this instead of joining each and to also account for any missing columns\n",
    "merged_2013_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), merged_dfs)\n",
    "\n",
    "# View newly merged DataFrame by year\n",
    "merged_2013_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to combine two monthly files for each month (12 months) in a given year\n",
    "# Merge the 2 monthly files on \"Plan ID\" and store data in merged data frame for each month\n",
    "# Combine all merged monthly files into a single DataFrame for each defined year (2013-2023)\n",
    "# Write the finals results to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import col, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merge_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine monthly contract and enrollment files in a given year using an outer join (similar approach to above)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#base_path = \"CMS_ENROLLMENT_FILES/2013\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2013\u001b[39m\n\u001b[1;32m----> 4\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_files\u001b[49m(base_path, year)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Function\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_files\u001b[39m(year):\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Start a spark session\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merge_files' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine monthly contract and enrollment files in a given year using an outer join (similar approach to above)\n",
    "#base_path = \"CMS_ENROLLMENT_FILES/2013\"\n",
    "year = 2013\n",
    "merged_df = merge_files(base_path, year)\n",
    "\n",
    "# Function\n",
    "def merge_files(year):\n",
    "\n",
    "    # Start a spark session\n",
    "    spark = SparkSession.builder.appName(f\"Merge_{year}\").getOrCreate()\n",
    "    \n",
    "    # Placeholder for data\n",
    "    merged_df = None\n",
    "\n",
    "    # Create for loop to loop through each month (01-12)\n",
    "    for month in range(1,13):\n",
    "        # Format month:\n",
    "        month_str = f\"{month:02d}\"\n",
    "\n",
    "        # File path\n",
    "        #contract_file = f\"{base_path}/{year}/CPSC_Contract_Info_{year}_{month_str}.csv\"\n",
    "        #enrollment_file = f\"{base_path}/{year}/CPSC_Enrollment_Info_{year}_{month_str}.csv\"\n",
    "        contract_file = f\"{year}/CPSC_Contract_Info_{year}_{month_str}.csv\"\n",
    "        enrollment_file = f\"{year}/CPSC_Enrollment_Info_{year}_{month_str}.csv\"\n",
    "\n",
    "        try:\n",
    "            # Read CSVs\n",
    "            contract_df = spark.read.option(\"header\", True).csv(contract_file)\n",
    "            enrollment_df = spark.read.option(\"header\", True).csv(enrollment_file)\n",
    "\n",
    "            # Standardize column types\n",
    "            #contract_df = contract_df.withColumn(\"Plan ID\", trim(col(\"Plan ID\")).cast(\"string\"))\n",
    "            #enrollment_df = enrollment_df.withColumn(\"Plan ID\", trim(col(\"Plan ID\")).cast(\"string\"))\n",
    "\n",
    "            # Perform outer join on \"Plan ID\"\n",
    "            monthly_merged = contract_df.join(enrollment_df, on=\"Plan ID\", how=\"outer\")\n",
    "\n",
    "            # Combine with previous months\n",
    "            if merged_df is None:\n",
    "                merged_df = monthly_merged\n",
    "            else:\n",
    "                merged_df = merged_df.unionByName(monthly_merged, allowMissingColumns=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {month_str} due to error: {e}\")\n",
    "\n",
    "\n",
    "    # Save final merged DataFrame\n",
    "    if merged_df:\n",
    "        output_path = f\"{base_path}/{year}/CPSC_Merged_Data_{year}.csv\"\n",
    "        merged_df.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "        print(f\"Final merged file saved to: {output_path}\")\n",
    "    else:\n",
    "        print(f\"No valid files found for {year}.\")\n",
    "        \n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check merged df\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged DataFrame as a CSV file\n",
    "# merged_2013_01_df.write.csv(\"CPSC_Merged_Data_2013_01\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop Spark session\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependancies\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import re\n",
    "import numpy as np\n",
    "import us\n",
    "from census import Census\n",
    "import us\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting variables\n",
    "startpath =\"_Star_Ratings_and_Display_Measures/\" #start of star rating path\n",
    "fallpath =\"_Star_Ratings_Fall_Release/\"#path for fall ratings\n",
    "cpath=\"_Part_C\"#path for part c data\n",
    "dpath=\"_Part_D\"#path for part d data\n",
    "finalpath =\"_Report_Card_Master_Table.xlsx\"#last part of star rating path\n",
    "firstfive = [\"Contract Number\",\"Organization Type\",\"Contract Name\",\"Organization Marketing Name\",\"Parent Organization\"]#name of first five columns\n",
    "lasttwo =[\"Year\",\"Overall\"] #name of what will be final 2 columns\n",
    "apikey = \"\"\n",
    "fields = ['NAME', 'B01001_007E', 'B01001_008E']#fields used for census data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clean Star Ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get measure star information\n",
    "#takes in path of the file\n",
    "#returns dataframe of the star information\n",
    "def get_measure_stars(path):\n",
    "    dfms = pd.read_excel(path,\"Measure_Stars\",header=2)#df of the raw information\n",
    "    dfms = dfms.iloc[1:]#remove first line of data\n",
    "    #loop through the columns after the first five\n",
    "    for x in range(5,len(dfms.keys())):\n",
    "        #remove letter number information from column name\n",
    "        dfms =dfms.rename(columns= {dfms.keys()[x] : re.split(r'\\d+:',dfms.keys()[x])[0]+re.split(r'\\d+:',dfms.keys()[x])[1]})\n",
    "    #loop through the first 5 columns to give them proper names\n",
    "    for x in range(5):\n",
    "        #rename the columns\n",
    "        dfms = dfms.rename(columns={f\"Unnamed: {x}\":firstfive[x]})\n",
    "    #return the data frame\n",
    "    return dfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get Domain star information\n",
    "#takes in path of the file\n",
    "#returns dataframe of the star information\n",
    "def get_domain_stars(path):\n",
    "    dfds = pd.read_excel(path,\"Domain_Stars\",header=1)\n",
    "    return dfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get summary star information\n",
    "#takes in path of the file\n",
    "#returns dataframe of the star information\n",
    "def get_summary_rating(path):\n",
    "    #read in data frame\n",
    "    dfsr = pd.read_excel(path,\"Summary_Rating\",header=1)\n",
    "    #remove the sanction deduction column\n",
    "    dfsr = dfsr.drop(columns=\"Sanction Deduction\",errors='ignore')\n",
    "    #retrun dataframe\n",
    "    return dfsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to combine the 3 dataframes and do basic cleanup on them used for early years\n",
    "#takes year of the function\n",
    "#returns the cleaned dataframe\n",
    "def get_early(y):\n",
    "    #build path for files\n",
    "    fullpath = f\"./Data/{y}{startpath}{y}{fallpath}{y}{finalpath}\"\n",
    "    #get measure stars info\n",
    "    dfmsf = get_measure_stars(fullpath)\n",
    "    #get domain star info\n",
    "    dfdsf = get_domain_stars(fullpath)\n",
    "    #get summary star info\n",
    "    dfsrf = get_summary_rating(fullpath)\n",
    "    #merge measure and domain stars\n",
    "    dff = pd.merge(dfmsf,dfdsf,on=firstfive,how='left')\n",
    "    #merge measure, domain, and summary star dataframes\n",
    "    dff = pd.merge(dff,dfsrf,on=firstfive,how='left')\n",
    "    #add year column\n",
    "    dff[\"Year\"] = f\"{y}\"\n",
    "    #rename columns\n",
    "    dff = dff.rename(columns={f\"{y} Part C Summary\":\"Part C Summary\",f\"{y} Overall\":\"Overall\",f\"{y} Part D Summary\":\"Part D Summary\"})\n",
    "    #drop unneeded columns\n",
    "    dff = dff.drop(columns = \"2017 Disaster %\",errors='ignore')\n",
    "    #return dataframe\n",
    "    return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to combine the 3 dataframes and do basic cleanup on them used for later years\n",
    "#takes year of the function\n",
    "#returns the cleaned dataframe\n",
    "def get_late(y):\n",
    "    #build path for files\n",
    "    fullpath = f\"./Data/{y}{startpath}{y}{finalpath}\"\n",
    "    #get measure stars info\n",
    "    dfmsc = get_measure_stars(fullpath)\n",
    "    #get domain star info\n",
    "    dfdsc = get_domain_stars(fullpath)\n",
    "    #get summary star info\n",
    "    dfsrc = get_summary_rating(fullpath)\n",
    "    #drop disaster columns\n",
    "    dfsrc = dfsrc.drop(columns=dfsrc.columns[[6,7]],axis = 1)\n",
    "    #merge measure and domain stars\n",
    "    df = pd.merge(dfmsc,dfdsc,on=firstfive,how='left')\n",
    "    #merge measure, domain, and summary star dataframes\n",
    "    df = pd.merge(df,dfsrc,on=firstfive,how='left')\n",
    "    #rename columns\n",
    "    df = df.rename(columns={f\"{y} Part C Summary\":\"Part C Summary\",f\"{y} Overall\":\"Overall\",f\"{y} Part D Summary\":\"Part D Summary\"})\n",
    "    #add year column\n",
    "    df[\"Year\"] = f\"{y}\"\n",
    "    #return dataframe\n",
    "    return    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to combine the dataframse for all of the years\n",
    "#no input\n",
    "#returns data frame with data from all years\n",
    "def standardize_data():\n",
    "    #set first year\n",
    "    year = 2014\n",
    "    #set years of data\n",
    "    years = [x for x in range(2015,2026)]\n",
    "    #set path for part C 2014        \n",
    "    fullpath = f\"./Data/{year}{startpath}{year}{fallpath}{year}{cpath}{finalpath}\"\n",
    "    #get measure star dataframe for part C 2014\n",
    "    dfmsc = get_measure_stars(fullpath)\n",
    "    #get domain star dataframe for part C 2014\n",
    "    dfdsc = get_domain_stars(fullpath)\n",
    "    #get summary star dataframe for part C 2014\n",
    "    dfsrc = get_summary_rating(fullpath)\n",
    "    #merge measure and domain star dataframes for part C 2014\n",
    "    df = pd.merge(dfmsc,dfdsc,on=firstfive,how='left')\n",
    "    #merge measure, domain, and summary star dataframes for part C 2014\n",
    "    df = pd.merge(df,dfsrc,on=firstfive,how='left')\n",
    "    #set path for part D 2014    \n",
    "    fullpath = f\"./Data/{year}{startpath}{year}{fallpath}{year}{dpath}{finalpath}\"\n",
    "    #get measure star dataframe for part D 2014\n",
    "    dfmsd = get_measure_stars(fullpath)\n",
    "    #get domain star dataframe for part D 2014\n",
    "    dfdsd = get_domain_stars(fullpath)\n",
    "    #get summary star dataframe for part D 2014\n",
    "    dfsrd = get_summary_rating(fullpath)\n",
    "    #merge measure and domain star dataframes for part D 2014\n",
    "    dfd = pd.merge(dfmsd,dfdsd,on=['Contract Number'],how='left')\n",
    "    #merge measure, domain, and summary star dataframes for part D 2014\n",
    "    dfd = pd.merge(dfd,dfsrd,on=['Contract Number'],how='left')\n",
    "    #drop duplicate columns\n",
    "    dfd = dfd.drop(columns=[\"Organization Type_x\",\"Contract Name_x\",\"Organization Marketing Name_x\",\"Parent Organization_x\",\"Organization Type_y\",\"Contract Name_y\",\"Organization Marketing Name_y\",\"Parent Organization_y\"])\n",
    "    #merge part C and D data\n",
    "    df = pd.merge(df,dfd,on=[\"Contract Number\",\"Organization Type\",\"Contract Name\",\"Organization Marketing Name\",\"Parent Organization\",\"SNP\"],how='left')\n",
    "    #rename columns\n",
    "    df = df.rename(columns={f\"{year} Part C Summary Rating\":\"Part C Summary\",f\"{year} Overall Rating\":\"Overall\",f\"{year} Part D Summary Rating\":\"Part D Summary\"})\n",
    "    #add year to  dataframes\n",
    "    df[\"Year\"] = \"2014\"\n",
    "    finaldf = df\n",
    "    #loop through years\n",
    "    for year in years:\n",
    "        #choose which merger to call if early call early otherwise call late\n",
    "        if (year <2020):\n",
    "            df = get_early(year)\n",
    "        else:\n",
    "            df = get_late(year)\n",
    "        #concatinate the dataframes into one big data frame\n",
    "        newdf = pd.concat([finaldf,df],axis=0,join='outer')\n",
    "        #change the dataframe so it can be reused\n",
    "        finaldf=newdf\n",
    "    #reset index of dataframe    \n",
    "    finaldf = finaldf.reset_index(drop=True)\n",
    "    #return final dataframe\n",
    "    return finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean the dataframe\n",
    "#takes in a dataframe\n",
    "#returns cleaned dataframe\n",
    "def clean_data(combodf1):\n",
    "    #loop through columns in dataframe\n",
    "    for x in range(len(combodf1.keys())):\n",
    "        #change datatypes to string\n",
    "        combodf1[combodf1.keys()[x]] = combodf1[combodf1.keys()[x]].astype(str)\n",
    "    #trim various typs of data to remove excess spaces\n",
    "    combodf1 = combodf1.replace('\\\\s*Plan\\\\s*too\\\\s*new\\\\s*to\\\\s*be\\\\s*measured\\\\s*','Plan too new to be measured',regex=True)\n",
    "    combodf1 = combodf1.replace('\\\\s*Plan\\\\s*too\\\\s*small\\\\s*to\\\\s*be\\\\s*measured\\\\s*','Plan too small to be measured',regex=True)\n",
    "    combodf1 = combodf1.replace('\\\\s*Plan\\\\s*not\\\\s*required\\\\s*to\\\\s*report\\\\s*measure\\\\s*','Plan not required to report measure',regex=True)\n",
    "    combodf1 = combodf1.replace('\\\\s*No\\\\s*data\\\\s*available\\\\s*','No data available',regex=True)\n",
    "    combodf1 = combodf1.replace('Nodata available','No data available',regex=True)\n",
    "    combodf1 = combodf1.replace('\\\\s*Not\\\\s*enough\\\\s*data\\\\s*available\\\\s*','Not enough data available',regex=True)\n",
    "    combodf1 = combodf1.replace('\\\\s*Benefit\\\\s*not\\\\s*offered\\\\s*by\\\\s*plan\\\\s*','Benefit not offered by plan',regex=True)\n",
    "    #change yes and not to true and false\n",
    "    combodf1 = combodf1.replace('\\\\s*Yes\\\\s*',True,regex=True)    \n",
    "    combodf1 = combodf1.replace('\\\\s+No\\\\s+',False,regex=True)\n",
    "    combodf1 = combodf1.replace('\\\\s+No',False,regex=True)\n",
    "    combodf1 = combodf1.replace('No\\\\s+',False,regex=True)\n",
    "    #replace nans that were created by adding columns with 0s\n",
    "    combodf1 = combodf1.replace('nan','0')\n",
    "    #return the dataframe\n",
    "    return combodf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to be used to create non numeric columns\n",
    "#takes in a dataframe item\n",
    "#returns either that item or a string\n",
    "def add_non_numeric_cols(item):\n",
    "    #attempt to set type to decimal\n",
    "    try:\n",
    "        #if it was able to make a decimal return numeric\n",
    "        val = float(item)\n",
    "        return \"Numeric\"\n",
    "    except:\n",
    "        #otherwise return the string\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean column of non numeric data\n",
    "#takes in dataframe item\n",
    "#returns 0 or the decimal version of item.\n",
    "def change_to_dec(item):\n",
    "    #attemp to set type to decimal\n",
    "    try:\n",
    "        #if successful return item\n",
    "        val = float(item)\n",
    "        return val\n",
    "    except:\n",
    "        #otherwise return 0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to bucket standardized enrollment\n",
    "#takes in dataframe item\n",
    "#returns bucket of the data\n",
    "def change_enrolled(item):\n",
    "    #first bucket\n",
    "    if item <.05:\n",
    "        return 0\n",
    "    #second bucket\n",
    "    elif item < .3:\n",
    "        return 1\n",
    "    #third bucket\n",
    "    elif item < .7:\n",
    "        return 2\n",
    "    #final bucket\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add enrollment data to a dataframe\n",
    "#takes in a dataframe\n",
    "#returns dataframe combined with enrollment data\n",
    "def combine_enrollment(newdf1):\n",
    "    #read in enrollment data\n",
    "    enrollmentcsv = pd.read_excel(\"./Data/Enrollment.xlsx\")\n",
    "    #read in state population\n",
    "    statepop = pd.read_csv(\"./CSVs/StatePopulations.csv\")\n",
    "    #map state fips and abbreviation\n",
    "    abfips = us.states.mapping('fips','abbr')\n",
    "    #change mapping into dataframe\n",
    "    abfips = pd.DataFrame.from_dict([abfips])\n",
    "    abfips = abfips.transpose()\n",
    "    abfips = abfips.reset_index()\n",
    "    abfips = abfips.rename(columns={\"index\":\"fips\",0:\"abbr\"})\n",
    "    #merge enrollment and state info\n",
    "    enrollab = pd.merge(enrollmentcsv,abfips,left_on='State',right_on='abbr')\n",
    "    enrollab = enrollab.drop(columns=\"abbr\")\n",
    "    enrollab['fips'] = enrollab['fips'].astype(int)\n",
    "    #merge enrollement with state population\n",
    "    enrollabpop = pd.merge(enrollab,statepop,left_on=['fips','Year'],right_on=['state','Year'])\n",
    "    enrollabpop = enrollabpop.drop(columns=['state'])\n",
    "    #find average enrollment in a year\n",
    "    testgroup = pd.DataFrame(enrollabpop.groupby(['Contract Number','State','Year'])['Total Enrollments by State'].mean())\n",
    "    testgroup = testgroup.reset_index()\n",
    "    testgroup['Total Enrollments by State'] = testgroup['Total Enrollments by State'].round(0)\n",
    "    #rename columns\n",
    "    testgroup = testgroup.rename(columns={\"Total Enrollments by State\":\"Avg Enrollment\"})\n",
    "    #merge average enrollment into enrollment\n",
    "    enrollwavg = pd.merge(testgroup,enrollabpop,on=['Contract Number', 'State','Year'])\n",
    "    #standardize enrollment for state population\n",
    "    enrollwavg[\"Standardized Enrollment\"] = enrollwavg['Avg Enrollment']/enrollwavg['Population Over 65']\n",
    "    #remove unneccesary columns and drop duplicates that removing columns caused\n",
    "    enrollwavg1 = enrollwavg.copy()\n",
    "    enrollwavg1 = enrollwavg1.drop(columns=['Month','Total Enrollments by State'])\n",
    "    enrollwavg1 = enrollwavg1.drop_duplicates()\n",
    "    datatomerge = enrollwavg1.groupby([\"Contract Number\",\"Year\"])['Standardized Enrollment'].mean()\n",
    "    datatomerge = datatomerge.reset_index()\n",
    "    #merge enrollment data and star rating dataframe\n",
    "    finaldf = pd.merge(newdf1,datatomerge,on=['Contract Number','Year'])\n",
    "    #bucket the standardized enrollment data\n",
    "    finaldf[\"Standardized Enrollment\"] = finaldf[\"Standardized Enrollment\"].apply(change_enrolled)\n",
    "    #change enrollment column to int\n",
    "    finaldf[\"Standardized Enrollment\"] = finaldf[\"Standardized Enrollment\"].astype(int)\n",
    "    #return new dataframe\n",
    "    return finaldf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to split numeric and non-numeric columns\n",
    "#takes in a dataframe\n",
    "#returns a dataframe with numeric and non-numeric columns\n",
    "def split_numeric_nonnumeric_data(combodf4):\n",
    "    #get list of columns\n",
    "    cols = combodf4.keys().to_list()\n",
    "    cols.append(\"Overall\")\n",
    "    #loop through first five columns\n",
    "    for col in firstfive:\n",
    "        cols.remove(col)\n",
    "    #loop through last 2 columns\n",
    "    for col in lasttwo:\n",
    "        cols.remove(col)\n",
    "    #loops througy columns in dataframe\n",
    "    for col in cols:\n",
    "        #add non-numeric columns\n",
    "        combodf4[col+\" Non-numeric\"] = combodf4[col].apply(add_non_numeric_cols)\n",
    "        #change old columns to numeric\n",
    "        combodf4[col]=combodf4[col].apply(change_to_dec)\n",
    "    newcols = []#placeholder for new column order\n",
    "    #loop through columns\n",
    "    for col in cols:\n",
    "        #add numeric column\n",
    "        newcols.append(col)\n",
    "        #add non-numeric column\n",
    "        newcols.append(col+\" Non-numeric\")\n",
    "    finalcols =[]#placeholder for final column order\n",
    "    #add columns to final columns\n",
    "    for col in firstfive:\n",
    "        finalcols.append(col)\n",
    "    finalcols.append(\"Year\")\n",
    "    for col in newcols:\n",
    "        finalcols.append(col)\n",
    "    #change order of columns in dataframe\n",
    "    newdf1 = combodf4[finalcols]\n",
    "    return newdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combodf = standardize_data()\n",
    "combodf = clean_data(combodf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combodf = standardize_data()\n",
    "combodf.to_csv(\"./CSVs/CombinedData.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsi\\AppData\\Local\\Temp\\ipykernel_37080\\2318447181.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combodf1 = combodf1.replace('\\\\s*No\\\\s*',False,regex=True)\n"
     ]
    }
   ],
   "source": [
    "combodf = clean_data(combodf)\n",
    "combodf.to_csv(\"./CSVs/CombinedDataCleaned.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1=split_numeric_nonnumeric_data(combodf)\n",
    "newdf1.to_csv(\"./CSVs/CombinedDataNoEnrollment.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pull Census Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create df of state populations\n",
    "#takes a list of population data and the year\n",
    "#returns a data fram with state populations and the year of the population\n",
    "def create_year_pop_df(ls,year):\n",
    "    #create the df\n",
    "    df = pd.DataFrame(ls)\n",
    "    #create population column\n",
    "    df['Population Over 65'] = df['B01001_007E']+df['B01001_008E']\n",
    "    #create year column\n",
    "    df['Year'] = year\n",
    "    #drop unneeded \"states\"\n",
    "    df.drop(df.index[df['NAME'] == 'District of Columbia'], inplace=True)\n",
    "    df.drop(df.index[df['NAME'] == 'Puerto Rico'], inplace=True)\n",
    "    #order columns and drop unneeded ones\n",
    "    dfother = df[['NAME','state','Population Over 65', 'Year']]\n",
    "    return dfother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion tocombine all of the year data frames into one dataframe\n",
    "#takes a list of population information\n",
    "#returns a combined dataframe\n",
    "def create_dfs(poy):\n",
    "    #set years of state data\n",
    "    years = [y for y in range(2014,2024)]\n",
    "    #create starting df\n",
    "    df1 = create_year_pop_df(poy[0],2014)\n",
    "    #loop through the years of dataframes\n",
    "    for x in range(1,len(years)):\n",
    "        #create dataframe\n",
    "        df2 = create_year_pop_df(poy[x],years[x])\n",
    "        #concatinate dataframes\n",
    "        df1 = pd.concat([df1,df2])\n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Census' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#create census instance\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m c \u001b[38;5;241m=\u001b[39m Census(apikey)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#create mapping of fips and abbreviation\u001b[39;00m\n\u001b[0;32m      4\u001b[0m abfips \u001b[38;5;241m=\u001b[39m us\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mmapping(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfips\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabbr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Census' is not defined"
     ]
    }
   ],
   "source": [
    "#create census instance\n",
    "c = Census(apikey)\n",
    "#create mapping of fips and abbreviation\n",
    "abfips = us.states.mapping('fips','abbr')\n",
    "popsoveryear = []#placeholder for data\n",
    "tracker = 0#keep track of iterations\n",
    "#loop through years for data\n",
    "for y in range(2014,2024):\n",
    "    #pull data from census\n",
    "    popsoveryear.append(c.acs5.get((fields),\n",
    "          {'for': 'state:*'},year = y))\n",
    "    #increment tracker\n",
    "    tracker+=1\n",
    "    #create the dataframe\n",
    "df8 = create_dfs(popsoveryear)\n",
    "#save dataframe\n",
    "df8.to_csv('./CSVs/StatePopulations.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrollment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import tensorflow as tf\n",
    "\n",
    "#read in data for tuner\n",
    "finaldf = pd.read_csv(\"./CSVs/FinalData.csv\")\n",
    "#drop catagorical columns\n",
    "finaldf1 = finaldf.copy()\n",
    "finaldf1 = finaldf1.drop(columns=[\"Contract Number\",\"Organization Type\",\"Contract Name\",\"Organization Marketing Name\",\"Parent Organization\"])\n",
    "#add dummies\n",
    "finaldf1 = pd.get_dummies(finaldf1)\n",
    "finaldf1['Standardized Enrollment'] = finaldf1['Standardized Enrollment']-1\n",
    "\n",
    "# Separate the y variable\n",
    "y = finaldf1[\"Standardized Enrollment\"]\n",
    "# Separate the X variable, the features\n",
    "X = finaldf1.drop(columns=\"Standardized Enrollment\")\n",
    "\n",
    "# Use sklearn to split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "# Create scaler instance\n",
    "X_scaler = skl.preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh','sigmoid','leaky_relu'])\n",
    "\n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=10,\n",
    "        step=2), activation=activation, input_dim=417))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 6)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=10,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "\n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tune_dir\\tensorworld\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Import the keras-tuner library\n",
    "import keras_tuner as kt\n",
    "tuner = kt.Hyperband(\n",
    "    create_model, \n",
    "    seed=42,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2,\n",
    "    directory=\"tune_dir\",\n",
    "    project_name=\"tensorworld\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'first_units': 1,\n",
       " 'num_layers': 4,\n",
       " 'units_0': 1,\n",
       " 'units_1': 9,\n",
       " 'units_2': 9,\n",
       " 'units_3': 3,\n",
       " 'units_4': 7,\n",
       " 'units_5': 7,\n",
       " 'tuner/epochs': 20,\n",
       " 'tuner/initial_epoch': 0,\n",
       " 'tuner/bracket': 0,\n",
       " 'tuner/round': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best model hyperparameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 - 0s - 8ms/step - accuracy: 0.7265 - loss: 0.2175\n",
      "Loss: 0.21748434007167816, Accuracy: 0.7265343070030212\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model against full test data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "best_model.save(\"./Models/BestKerasModel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data to run model against\n",
    "finaldf = pd.read_csv(\"./CSVs/FinalData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop catagorical columns\n",
    "finaldf1 = finaldf.copy()\n",
    "finaldf1 = finaldf1.drop(columns=[\"Contract Number\",\"Organization Type\",\"Contract Name\",\"Organization Marketing Name\",\"Parent Organization\"])\n",
    "finaldf1 = pd.get_dummies(finaldf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the y variable\n",
    "y = finaldf1[\"Standardized Enrollment\"]\n",
    "# Separate the X variable, the features\n",
    "X = finaldf1.drop(columns=\"Standardized Enrollment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random forest classifier instance\n",
    "rf_model = RandomForestClassifier(n_estimators=500, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsi\\AppData\\Local\\Temp\\ipykernel_11872\\1622860369.py:2: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  rf_model = rf_model.fit(X_train, y_train.ravel())\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "rf_model = rf_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create predictions\n",
    "predictions = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.7102888086642599\n"
     ]
    }
   ],
   "source": [
    "#test accuracy of the model\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy Score : {acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.027039949114824356,\n",
       "  'C Osteoporosis Management in Women who had a Fracture'),\n",
       " (0.02527450134872629, 'C Rheumatoid Arthritis Management'),\n",
       " (0.02231341374282398, 'D Members Choosing to Leave the Plan'),\n",
       " (0.018856959507036974, 'D Appeals Upheld'),\n",
       " (0.0185580470284319, 'C Reviewing Appeals Decisions'),\n",
       " (0.017216119381393977,\n",
       "  'C Osteoporosis Management in Women who had a Fracture Non-numeric_Numeric'),\n",
       " (0.015732643930078432, 'D Getting Needed Prescription Drugs'),\n",
       " (0.01521858061400955, 'DD3: Member Experience with the Drug Plan'),\n",
       " (0.015171489728994788, 'HD5: Health Plan Customer Service'),\n",
       " (0.01516747825831993, 'C Care Coordination')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#find importance of features\n",
    "importances = rf_model.feature_importances_\n",
    "#order features based on importance\n",
    "importances_sorted = sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)\n",
    "importances_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight</th>\n",
       "      <th>Measurement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027040</td>\n",
       "      <td>C Osteoporosis Management in Women who had a F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025275</td>\n",
       "      <td>C Rheumatoid Arthritis Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022313</td>\n",
       "      <td>D Members Choosing to Leave the Plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018857</td>\n",
       "      <td>D Appeals Upheld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018558</td>\n",
       "      <td>C Reviewing Appeals Decisions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.017216</td>\n",
       "      <td>C Osteoporosis Management in Women who had a F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015733</td>\n",
       "      <td>D Getting Needed Prescription Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.015219</td>\n",
       "      <td>DD3: Member Experience with the Drug Plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.015171</td>\n",
       "      <td>HD5: Health Plan Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.015167</td>\n",
       "      <td>C Care Coordination</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Weight                                        Measurement\n",
       "0  0.027040  C Osteoporosis Management in Women who had a F...\n",
       "1  0.025275                  C Rheumatoid Arthritis Management\n",
       "2  0.022313               D Members Choosing to Leave the Plan\n",
       "3  0.018857                                   D Appeals Upheld\n",
       "4  0.018558                      C Reviewing Appeals Decisions\n",
       "5  0.017216  C Osteoporosis Management in Women who had a F...\n",
       "6  0.015733                D Getting Needed Prescription Drugs\n",
       "7  0.015219          DD3: Member Experience with the Drug Plan\n",
       "8  0.015171                  HD5: Health Plan Customer Service\n",
       "9  0.015167                                C Care Coordination"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create dataframe of importances\n",
    "importancedf = pd.DataFrame(importances_sorted)\n",
    "#label columns\n",
    "importancedf = importancedf.rename(columns={0:\"Weight\",1:\"Measurement\"})\n",
    "importancedf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Models/RandomForestEnrollment.joblib']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joblib.dump(rf_model,\"./Models/RandomForestEnrollment.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export wieght of columns\n",
    "importancedf.to_csv(\"./CSVs/RandomforestWeights.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combine_enrollment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m finaldf \u001b[38;5;241m=\u001b[39m combine_enrollment(newdf1)\n\u001b[0;32m      2\u001b[0m finaldf\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./CSVs/FinalData.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combine_enrollment' is not defined"
     ]
    }
   ],
   "source": [
    "finaldf = combine_enrollment(newdf1)\n",
    "finaldf.to_csv(\"./CSVs/FinalData.csv\",header=True,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
